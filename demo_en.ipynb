{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d23e9e1",
   "metadata": {},
   "source": [
    "# üé¨ Video Moment Retrieval Demo with Switch-NET\n",
    "\n",
    "This is a demo interface for video moment retrieval based on the Switch-NET model.\n",
    "\n",
    "- **Model**: Switch-NET (Loop Decoder DETR)\n",
    "- **Function**: Locate relevant moments in a video given a natural language query\n",
    "- **Interface**: Gradio interactive UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484cd7f",
   "metadata": {},
   "source": [
    "## 1. Import dependencies\n",
    "\n",
    "Import required libraries: PyTorch, Gradio, NumPy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662e4ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fyp/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/fyp/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <BC9ECF8E-76F4-3E60-B608-FAB81953710D> /opt/anaconda3/envs/fyp/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <AEDB2D9B-AE02-3964-90EC-49E2AD5A10A1> /opt/anaconda3/envs/fyp/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.4.1\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import tempfile\n",
    "import subprocess\n",
    "import ast\n",
    "import nbformat\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict, Tuple, Type, Any\n",
    "import copy\n",
    "import math\n",
    "import inspect\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import gradio as gr\n",
    "import clip\n",
    "from PIL import Image\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    cv2 = None\n",
    "\n",
    "PROJECT_ROOT = \"/Users/shuqi/Desktop/FYP\"\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cache for CLIP components to avoid reloading\n",
    "_clip_components = {\n",
    "    \"model\": None,\n",
    "    \"preprocess\": None,\n",
    "    \"device\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06f9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global feature cache to reuse extracted video features across queries\n",
    "MAX_CACHED_VIDEOS = 5\n",
    "_feature_cache: \"OrderedDict[Tuple, Dict[str, Any]]\" = OrderedDict()\n",
    "\n",
    "def _make_cache_key(video_path: str, max_v_l: int, vid_dim: int, clip_duration: Optional[float]) -> Optional[Tuple]:\n",
    "    \"\"\"Build a stable cache key for the given video + model configuration.\"\"\"\n",
    "    if not video_path:\n",
    "        return None\n",
    "    if vid_dim == 514:\n",
    "        return (video_path, max_v_l, \"clip_only\")\n",
    "    duration_value = 0.0 if clip_duration is None else float(clip_duration)\n",
    "    return (video_path, max_v_l, round(duration_value, 4), vid_dim)\n",
    "\n",
    "def _cache_get(key: Optional[Tuple]) -> Optional[Dict[str, Any]]:\n",
    "    if key is None:\n",
    "        return None\n",
    "    entry = _feature_cache.get(key)\n",
    "    if entry is None:\n",
    "        return None\n",
    "    _feature_cache.move_to_end(key, last=True)\n",
    "    return entry\n",
    "\n",
    "def _cache_put(key: Optional[Tuple], entry: Dict[str, Any]) -> None:\n",
    "    if key is None:\n",
    "        return\n",
    "    _feature_cache[key] = entry\n",
    "    _feature_cache.move_to_end(key, last=True)\n",
    "    while len(_feature_cache) > MAX_CACHED_VIDEOS:\n",
    "        evicted_key, _ = _feature_cache.popitem(last=False)\n",
    "        print(f\"[Feature Cache] Evicted cached features for {evicted_key}\")\n",
    "\n",
    "def get_cached_video_features(key: Optional[Tuple], vid_dim: int) -> Optional[Dict[str, Any]]:\n",
    "    entry = _cache_get(key)\n",
    "    if entry is None:\n",
    "        return None\n",
    "    if entry.get(\"vid_dim\") != vid_dim:\n",
    "        return None\n",
    "    return entry\n",
    "\n",
    "def cache_video_features(key: Optional[Tuple], fused_features: np.ndarray, fused_valid_len: int, vid_dim: int) -> None:\n",
    "    if key is None:\n",
    "        return\n",
    "    payload = {\n",
    "        \"fused_features\": np.array(fused_features, copy=True),\n",
    "        \"fused_valid_len\": int(fused_valid_len),\n",
    "        \"vid_dim\": int(vid_dim),\n",
    "    }\n",
    "    _cache_put(key, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39413d1e",
   "metadata": {},
   "source": [
    "## 2. Define Switch-NET model architecture\n",
    "\n",
    "This section defines the Switch-NET model structure, including:\n",
    "- Position Encoding\n",
    "- Multi-Head Attention\n",
    "- Transformer Encoder/Decoder\n",
    "- The main Switch-NET model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b98820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Notebook Loader] Skipping statement due to FileNotFoundError: [Errno 2] No such file or directory: 'output/models/best_model.pth'\n",
      "[Notebook Loader] Skipping statement due to NameError: name 'test_loader' is not defined\n",
      "Loaded Switch-Net definitions from training notebook (definition blocks executed: 112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuqi/Desktop/FYP/switch-net.ipynb:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  \"id\": \"70efac55\",\n"
     ]
    }
   ],
   "source": [
    "def load_Switch_net_from_notebook(notebook_path: str) -> Tuple[int, Type[nn.Module]]:\n",
    "    \"\"\"\n",
    "    Execute definition cells from the training notebook to populate Switch-Net classes.\n",
    "\n",
    "    Only top-level imports, assignments, functions, and classes are executed to avoid\n",
    "    running training loops or dataset instantiation code.\n",
    "    Returns a tuple of (executed_blocks, resolved_model_class).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(notebook_path):\n",
    "        raise FileNotFoundError(f\"Notebook not found: {notebook_path}\")\n",
    "    \n",
    "    nb_data = nbformat.read(notebook_path, as_version=4)\n",
    "    exec_globals = globals()\n",
    "    executed = 0\n",
    "    \n",
    "    allowed_nodes = (\n",
    "        ast.Import,\n",
    "        ast.ImportFrom,\n",
    "        ast.FunctionDef,\n",
    "        ast.AsyncFunctionDef,\n",
    "        ast.ClassDef,\n",
    "        ast.Assign,\n",
    "        ast.AnnAssign,\n",
    "    )\n",
    "    \n",
    "    def execute_node(node: ast.AST):\n",
    "        nonlocal executed\n",
    "        module = ast.Module(body=[node], type_ignores=[])\n",
    "        ast.fix_missing_locations(module)\n",
    "        try:\n",
    "            code_obj = compile(module, notebook_path, \"exec\")\n",
    "            exec(code_obj, exec_globals)\n",
    "        except NameError as err:\n",
    "            print(f\"[Notebook Loader] Skipping statement due to NameError: {err}\")\n",
    "            return\n",
    "        except Exception as err:\n",
    "            print(f\"[Notebook Loader] Skipping statement due to {err.__class__.__name__}: {err}\")\n",
    "            return\n",
    "        executed += 1\n",
    "    \n",
    "    for cell in nb_data.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "        source = cell.get(\"source\", \"\")\n",
    "        if isinstance(source, list):\n",
    "            source = \"\".join(source)\n",
    "        if not source.strip():\n",
    "            continue\n",
    "        try:\n",
    "            tree = ast.parse(source, filename=notebook_path)\n",
    "        except SyntaxError:\n",
    "            continue\n",
    "        for node in tree.body:\n",
    "            if isinstance(node, allowed_nodes):\n",
    "                execute_node(node)\n",
    "            elif isinstance(node, ast.Try):\n",
    "                safe = all(isinstance(stmt, allowed_nodes) for stmt in node.body)\n",
    "                if safe:\n",
    "                    for stmt in node.body:\n",
    "                        execute_node(stmt)\n",
    "            # Skip other node types (loops, with-statements, main guards, etc.)\n",
    "    \n",
    "    resolved_model = exec_globals.get(\"SwitchNet\") or exec_globals.get(\"LD_DETR\")\n",
    "    if resolved_model is None:\n",
    "        raise RuntimeError(\"Switch-Net definition not found after executing notebook definitions.\")\n",
    "    \n",
    "    exec_globals[\"SwitchNet\"] = resolved_model\n",
    "    \n",
    "    return executed, resolved_model\n",
    "\n",
    "MODEL_SOURCE_NOTEBOOK = os.path.join(PROJECT_ROOT, \"switch-net.ipynb\")\n",
    "_blocks_executed, SwitchNET = load_Switch_net_from_notebook(MODEL_SOURCE_NOTEBOOK)\n",
    "print(f\"Loaded Switch-Net definitions from training notebook (definition blocks executed: {_blocks_executed})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e9c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch-NET model components loaded from training notebook definitions\n"
     ]
    }
   ],
   "source": [
    "print(\"Switch-NET model components loaded from training notebook definitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb75e53",
   "metadata": {},
   "source": [
    "## 3. Model loading function\n",
    "\n",
    "Define a helper to load a trained Switch-NET checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d3e412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading function defined!\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model(checkpoint_path: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Load a trained Switch-Net checkpoint and return the model plus its resolved config.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Filesystem path to the serialized `.pth` checkpoint.\n",
    "        device: Target device onto which the model should be moved.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (model, resolved_config_dict). The model is ready for eval().\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    \n",
    "    load_kwargs = {\"map_location\": device}\n",
    "    checkpoint = None\n",
    "    if \"weights_only\" in inspect.signature(torch.load).parameters:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, weights_only=True, **load_kwargs)\n",
    "        except Exception as exc:\n",
    "            warnings.warn(\n",
    "                f\"Falling back to torch.load(weights_only=False) because safe loading failed with: {exc}\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "            checkpoint = torch.load(checkpoint_path, weights_only=False, **load_kwargs)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path, **load_kwargs)\n",
    "    \n",
    "    raw_config = checkpoint.get(\"config\", {})\n",
    "    if hasattr(raw_config, \"_asdict\"):\n",
    "        raw_config = raw_config._asdict()\n",
    "    elif hasattr(raw_config, \"__dict__\"):\n",
    "        raw_config = vars(raw_config)\n",
    "    \n",
    "    if not isinstance(raw_config, dict):\n",
    "        raw_config = {}\n",
    "    \n",
    "    def to_builtin(value):\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            return value.item()\n",
    "        return value\n",
    "    \n",
    "    field_aliases = {\n",
    "        \"txt_dim\": ([\"txt_dim\", \"query_dim\", \"text_dim\"], 512),\n",
    "        \"vid_dim\": ([\"vid_dim\", \"video_dim\", \"video_feature_dim\"], 512),\n",
    "        \"hidden_dim\": ([\"hidden_dim\", \"d_model\"], 256),\n",
    "        \"num_queries\": ([\"num_queries\"], 10),\n",
    "        \"aux_loss\": ([\"aux_loss\"], True),\n",
    "        \"position_embedding\": ([\"position_embedding\"], \"sine\"),\n",
    "        \"max_v_l\": ([\"max_v_l\", \"max_video_len\"], 75),\n",
    "        \"max_q_l\": ([\"max_q_l\", \"max_query_len\"], 32),\n",
    "        \"span_loss_type\": ([\"span_loss_type\"], \"l1\"),\n",
    "        \"use_txt_pos\": ([\"use_txt_pos\"], False),\n",
    "        \"aud_dim\": ([\"aud_dim\"], 0),\n",
    "        \"queue_length\": ([\"queue_length\"], 65536),\n",
    "        \"momentum\": ([\"momentum\"], 0.995),\n",
    "        \"distillation_coefficient\": ([\"distillation_coefficient\"], 0.4),\n",
    "        \"num_v2t_encoder_layers\": ([\"num_v2t_encoder_layers\"], 2),\n",
    "        \"num_encoder1_layers\": ([\"num_encoder1_layers\"], 2),\n",
    "        \"num_convolutional_blocks\": ([\"num_convolutional_blocks\"], 5),\n",
    "        \"num_encoder2_layers\": ([\"num_encoder2_layers\"], 2),\n",
    "        \"num_decoder_layers\": ([\"num_decoder_layers\"], 2),\n",
    "        \"num_decoder_loops\": ([\"num_decoder_loops\"], 3),\n",
    "        \"clip_len\": ([\"clip_len\"], 2),\n",
    "    }\n",
    "    \n",
    "    def resolve_field(keys, default_value):\n",
    "        for key in keys:\n",
    "            if key in raw_config and raw_config[key] is not None:\n",
    "                return to_builtin(raw_config[key])\n",
    "        return default_value\n",
    "    \n",
    "    model_args = {\n",
    "        field: resolve_field(keys, default)\n",
    "        for field, (keys, default) in field_aliases.items()\n",
    "    }\n",
    "    \n",
    "    model = SwitchNET(**model_args)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    load_result = model.load_state_dict(state_dict, strict=False)\n",
    "    if load_result.missing_keys:\n",
    "        print(f\"Missing keys during load: {load_result.missing_keys[:5]}\")\n",
    "        if len(load_result.missing_keys) > 5:\n",
    "            print(\"(truncated)\")\n",
    "    if load_result.unexpected_keys:\n",
    "        print(f\"Unexpected keys during load: {load_result.unexpected_keys[:5]}\")\n",
    "        if len(load_result.unexpected_keys) > 5:\n",
    "            print(\"(truncated)\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    print(f\"Resolved config: {model_args}\")\n",
    "    print(f\"Trained for {checkpoint.get('epoch', 'unknown')} epochs\")\n",
    "    \n",
    "    return model, model_args\n",
    "\n",
    "print(\"Model loading function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97f549",
   "metadata": {},
   "source": [
    "## 4. Feature extraction functions\n",
    "This section gathers the encoding modules used during inference:\n",
    "- **CLIP text/image features**: reuse the same ViT-B/32 used during training to align language and visual semantics;\n",
    "- **TEF (Temporal Endpoint Features)**: provide normalized start/end position information to help the model interpret time indices;\n",
    "- **SlowFast video features**: capture rich spatio-temporal motion representations, used for the 2818-d multimodal input configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a9d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions ready (CLIP + TEF)\n"
     ]
    }
   ],
   "source": [
    "CLIP_MODEL_NAME = \"ViT-B/32\"\n",
    "TEF_DIM = 2\n",
    "\n",
    "def get_clip_components():\n",
    "    \"\"\"Load and cache the CLIP model and preprocess pipeline.\"\"\"\n",
    "    if _clip_components[\"model\"] is None:\n",
    "        clip_device = device\n",
    "        model, preprocess = clip.load(CLIP_MODEL_NAME, device=device, jit=False)\n",
    "        model.eval()\n",
    "        _clip_components[\"model\"] = model\n",
    "        _clip_components[\"preprocess\"] = preprocess\n",
    "        _clip_components[\"device\"] = clip_device\n",
    "        print(f\"Loaded CLIP model ({CLIP_MODEL_NAME}) on {clip_device}\")\n",
    "    return _clip_components[\"model\"], _clip_components[\"preprocess\"], _clip_components[\"device\"]\n",
    "\n",
    "def build_tef_features(num_clips: int) -> np.ndarray:\n",
    "    \"\"\"Construct temporal endpoint features (TEF) matching training logic.\"\"\"\n",
    "    if num_clips <= 0:\n",
    "        return np.zeros((0, TEF_DIM), dtype=np.float32)\n",
    "    positions = np.arange(0, num_clips, dtype=np.float32) / float(num_clips)\n",
    "    tef_start = positions\n",
    "    tef_end = positions + 1.0 / float(num_clips)\n",
    "    tef = np.stack([tef_start, tef_end], axis=1)\n",
    "    return tef.astype(np.float32)\n",
    "\n",
    "def extract_video_clip_features(video_path: str, num_clips: int = 64, feature_dim: int = 512) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Extract CLIP frame features from a video file and return features with valid length.\"\"\"\n",
    "    if cv2 is None:\n",
    "        raise RuntimeError(\"OpenCV (cv2) is required for video processing but is not available.\")\n",
    "    \n",
    "    model, preprocess, clip_device = get_clip_components()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Unable to open video file: {video_path}\")\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames <= 0:\n",
    "        total_frames = num_clips\n",
    "    \n",
    "    frame_indices = np.linspace(0, max(total_frames - 1, 0), num=num_clips, dtype=int)\n",
    "    collected = []\n",
    "    \n",
    "    for frame_idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_idx))\n",
    "        success, frame = cap.read()\n",
    "        if not success or frame is None:\n",
    "            continue\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame_rgb)\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(clip_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            clip_feat = model.encode_image(image_tensor)\n",
    "            clip_feat = clip_feat / clip_feat.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        collected.append(clip_feat.squeeze(0).cpu().float().numpy())\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not collected:\n",
    "        return np.zeros((num_clips, feature_dim), dtype=np.float32), 0\n",
    "    \n",
    "    features = np.vstack(collected)\n",
    "    valid_length = min(features.shape[0], num_clips)\n",
    "    \n",
    "    if features.shape[0] < num_clips:\n",
    "        padding = np.zeros((num_clips - features.shape[0], feature_dim), dtype=np.float32)\n",
    "        features = np.concatenate([features, padding], axis=0)\n",
    "    \n",
    "    return features[:num_clips].astype(np.float32), valid_length\n",
    "\n",
    "def extract_query_clip_features(query_text: str, seq_len: int = 32, feature_dim: int = 512) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Extract per-token CLIP text features and return features with valid token length.\"\"\"\n",
    "    model, _, clip_device = get_clip_components()\n",
    "    if not query_text or query_text.strip() == \"\":\n",
    "        return np.zeros((seq_len, feature_dim), dtype=np.float32), 0\n",
    "    \n",
    "    tokens = clip.tokenize([query_text], truncate=True).to(clip_device)\n",
    "    with torch.no_grad():\n",
    "        x = model.token_embedding(tokens).type(model.dtype)\n",
    "        x = x + model.positional_embedding.type(model.dtype)\n",
    "        x = x.permute(1, 0, 2)  # (context_length, batch, dim)\n",
    "        x = model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # (batch, context_length, dim)\n",
    "        x = model.ln_final(x)\n",
    "        text_features = x[0].float().cpu().numpy()\n",
    "    \n",
    "    valid_length = int((tokens[0] != 0).sum().item())\n",
    "    valid_length = min(valid_length, seq_len)\n",
    "    \n",
    "    if text_features.shape[1] != feature_dim:\n",
    "        raise ValueError(f\"Expected text feature dim {feature_dim}, got {text_features.shape[1]}\")\n",
    "    \n",
    "    text_features = text_features[:seq_len]\n",
    "    if text_features.shape[0] < seq_len:\n",
    "        padding = np.zeros((seq_len - text_features.shape[0], feature_dim), dtype=np.float32)\n",
    "        text_features = np.concatenate([text_features, padding], axis=0)\n",
    "    \n",
    "    return text_features.astype(np.float32), valid_length\n",
    "\n",
    "print(\"Feature extraction functions ready (CLIP + TEF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedae816",
   "metadata": {},
   "source": [
    "## 4.1 SlowFast video feature extraction\n",
    "Use a pretrained SlowFast R50 to further encode the video's spatio-temporal dynamics and complement CLIP frame features' lack of motion information.\n",
    "For each sampled clip we:\n",
    "- perform uniform temporal sampling, spatial scaling and center crop to match SlowFast pretraining distribution;\n",
    "- build Slow/Fast dual pathways to leverage SlowFast's multi-rate perception;\n",
    "- register a forward hook at the model head to capture a 2304-d feature vector from the pooling/projection layer;\n",
    "- zero-pad when clips are missing so the final tensor shape is `(num_clips, 2304)` for easy concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2e9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fyp/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.models.hub import slowfast_r50\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "\n",
    "_slowfast_state = {\"model\": None, \"device\": None}\n",
    "\n",
    "SLOWFAST_ALPHA = 4\n",
    "SLOWFAST_NUM_FRAMES = 32\n",
    "SLOWFAST_SHORT_SIDE = 256\n",
    "SLOWFAST_CROP_SIZE = 256\n",
    "SLOWFAST_CLIP_SECONDS = 2.0\n",
    "SLOWFAST_MEAN = [0.45, 0.45, 0.45],\n",
    "SLOWFAST_STD = [0.225, 0.225, 0.225],\n",
    "SLOWFAST_FEATURE_DIM = 2304\n",
    "\n",
    "def _resize_short_side(frames: torch.Tensor, short_side: int) -> torch.Tensor:\n",
    "    \"\"\"Resize video frames so that the shorter spatial side equals `short_side`.\"\"\"\n",
    "    if short_side <= 0:\n",
    "        return frames\n",
    "    c, t, h, w = frames.shape\n",
    "    if h == 0 or w == 0:\n",
    "        return frames\n",
    "    scale = short_side / min(h, w)\n",
    "    target_h = max(int(round(h * scale)), 1)\n",
    "    target_w = max(int(round(w * scale)), 1)\n",
    "    frames_btchw = frames.permute(1, 0, 2, 3)  # (t, c, h, w)\n",
    "    frames_resized = F.interpolate(\n",
    "        frames_btchw,\n",
    "        size=(target_h, target_w),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    return frames_resized.permute(1, 0, 2, 3)  # back to (c, t, h, w)\n",
    "\n",
    "def _center_crop_video(frames: torch.Tensor, size: int) -> torch.Tensor:\n",
    "    \"\"\"Center crop the spatial dimensions to `size`. Pads if needed.\"\"\"\n",
    "    if size <= 0:\n",
    "        return frames\n",
    "    c, t, h, w = frames.shape\n",
    "    if h < size or w < size:\n",
    "        pad_h = max(size - h, 0)\n",
    "        pad_w = max(size - w, 0)\n",
    "        frames = frames.permute(1, 0, 2, 3)\n",
    "        frames = F.pad(frames, (0, pad_w, 0, pad_h))  # pad W then H\n",
    "        frames = frames.permute(1, 0, 2, 3)\n",
    "        c, t, h, w = frames.shape\n",
    "    top = max((h - size) // 2, 0)\n",
    "    left = max((w - size) // 2, 0)\n",
    "    return frames[:, :, top:top + size, left:left + size]\n",
    "\n",
    "def _normalize_video(frames: torch.Tensor, mean, std) -> torch.Tensor:\n",
    "    mean_tensor = torch.tensor(mean, dtype=frames.dtype, device=frames.device).view(-1, 1, 1, 1)\n",
    "    std_tensor = torch.tensor(std, dtype=frames.dtype, device=frames.device).view(-1, 1, 1, 1)\n",
    "    return (frames - mean_tensor) / std_tensor\n",
    "\n",
    "def _prepare_slowfast_clip(frames: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply temporal sampling, resize, crop, and normalize for SlowFast.\"\"\"\n",
    "    sampler = UniformTemporalSubsample(SLOWFAST_NUM_FRAMES)\n",
    "    frames = sampler(frames)\n",
    "    frames = frames.float() / 255.0\n",
    "    frames = _resize_short_side(frames, SLOWFAST_SHORT_SIDE)\n",
    "    frames = _center_crop_video(frames, SLOWFAST_CROP_SIZE)\n",
    "    frames = _normalize_video(frames, SLOWFAST_MEAN, SLOWFAST_STD)\n",
    "    return frames\n",
    "\n",
    "def _pack_slowfast_pathway(frames: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"Create the slow and fast pathways expected by SlowFast.\"\"\"\n",
    "    if frames.dim() != 4:\n",
    "        raise ValueError(\"Expected video tensor with shape (C, T, H, W)\")\n",
    "    fast_pathway = frames\n",
    "    num_slow_frames = max(int(np.ceil(frames.shape[1] / SLOWFAST_ALPHA)), 1)\n",
    "    slow_indices = torch.linspace(\n",
    "        0,\n",
    "        frames.shape[1] - 1,\n",
    "        steps=num_slow_frames,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    slow_pathway = torch.index_select(fast_pathway, 1, slow_indices.clamp(max=frames.shape[1] - 1))\n",
    "    return [slow_pathway, fast_pathway]\n",
    "\n",
    "def get_slowfast_backbone(target_device: torch.device = device) -> torch.nn.Module:\n",
    "    \"\"\"Load (and cache) the pretrained SlowFast backbone.\"\"\"\n",
    "    global _slowfast_state\n",
    "    cached = _slowfast_state\n",
    "    if cached[\"model\"] is not None and cached[\"device\"] == target_device:\n",
    "        return cached[\"model\"]\n",
    "    model = slowfast_r50(pretrained=True)\n",
    "    model = model.to(target_device)\n",
    "    model.eval()\n",
    "    _slowfast_state[\"model\"] = model\n",
    "    _slowfast_state[\"device\"] = target_device\n",
    "    return model\n",
    "\n",
    "def extract_slowfast_video_features(\n",
    "    video_path: str,\n",
    "    num_clips: int = 32,\n",
    "    clip_duration: float = SLOWFAST_CLIP_SECONDS,\n",
    "    target_device: torch.device = device,\n",
    "    quiet: bool = False,\n",
    " ) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Extract SlowFast features for uniformly sampled clips along a video.\n",
    "\n",
    "    Returns an array of shape (num_clips, 2304) and the number of valid clips.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video not found: {video_path}\")\n",
    "    model = get_slowfast_backbone(target_device)\n",
    "    encoded = EncodedVideo.from_path(video_path)\n",
    "    duration = encoded.duration if encoded.duration is not None else num_clips * clip_duration\n",
    "    if duration is None or duration <= 0:\n",
    "        duration = num_clips * clip_duration\n",
    "    clip_centers = np.linspace(\n",
    "        clip_duration / 2.0,\n",
    "        max(duration - clip_duration / 2.0, clip_duration / 2.0),\n",
    "        num=num_clips,\n",
    "    )\n",
    "\n",
    "    collected: List[np.ndarray] = []\n",
    "    valid = 0\n",
    "    feature_buffer: List[torch.Tensor] = []\n",
    "    \n",
    "    head_block = model.blocks[-1] if hasattr(model, \"blocks\") else None\n",
    "    pool_module = getattr(head_block, \"pool\", None) if head_block is not None else None\n",
    "    use_proj_inputs = False\n",
    "    if pool_module is None:\n",
    "        pool_module = getattr(head_block, \"proj\", None) if head_block is not None else None\n",
    "        use_proj_inputs = True if pool_module is not None else False\n",
    "    if pool_module is None:\n",
    "        raise RuntimeError(\"Unable to locate SlowFast pooling/projection module for feature capture.\")\n",
    "    \n",
    "    def _capture_hook(_, hook_inputs, hook_output):\n",
    "        if use_proj_inputs:\n",
    "            captured = hook_inputs[0].detach().cpu()\n",
    "            if captured.dim() > 2:\n",
    "                reduce_dims = tuple(range(2, captured.dim()))\n",
    "                captured = captured.mean(dim=reduce_dims)\n",
    "            feature_buffer.append(captured)\n",
    "        else:\n",
    "            feature_buffer.append(hook_output.detach().cpu())\n",
    "\n",
    "    hook_handle = pool_module.register_forward_hook(_capture_hook)\n",
    "    try:\n",
    "        for center in clip_centers:\n",
    "            start_sec = max(float(center - clip_duration / 2.0), 0.0)\n",
    "            end_sec = min(float(center + clip_duration / 2.0), duration)\n",
    "            clip = encoded.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "            if clip is None or \"video\" not in clip:\n",
    "                continue\n",
    "            frames = clip[\"video\"]\n",
    "            if frames.numel() == 0:\n",
    "                continue\n",
    "            frames = _prepare_slowfast_clip(frames)\n",
    "            pathways = _pack_slowfast_pathway(frames)\n",
    "            pathway_inputs = [p.unsqueeze(0).to(target_device) for p in pathways]\n",
    "            with torch.no_grad():\n",
    "                _ = model(pathway_inputs)\n",
    "            if not feature_buffer:\n",
    "                continue\n",
    "            pooled = feature_buffer.pop()\n",
    "            if pooled.dim() > 2:\n",
    "                pooled = pooled.flatten(start_dim=1)\n",
    "            collected.append(pooled.squeeze(0).numpy().astype(np.float32))\n",
    "            valid += 1\n",
    "    finally:\n",
    "        hook_handle.remove()\n",
    "\n",
    "    if not collected:\n",
    "        if not quiet:\n",
    "            print(\"SlowFast failed to extract valid clips, returning zero vectors.\")\n",
    "        return np.zeros((num_clips, SLOWFAST_FEATURE_DIM), dtype=np.float32), 0\n",
    "\n",
    "    features = np.stack(collected)\n",
    "    if features.shape[0] < num_clips:\n",
    "        padding = np.zeros((num_clips - features.shape[0], features.shape[1]), dtype=np.float32)\n",
    "        features = np.concatenate([features, padding], axis=0)\n",
    "\n",
    "    return features[:num_clips], valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fa4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, os, glob\n",
    "# cache_dir = os.path.join(torch.hub.get_dir(), \"checkpoints\")\n",
    "# for path in glob.glob(os.path.join(cache_dir, \"SLOWFAST*\")):\n",
    "#     print(f\"Removing {path}\")\n",
    "#     os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c69b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test: extract SlowFast features for a given video\n",
    "# test_video_path = \"/Users/shuqi/Desktop/FYP/data/videos/0A8ZT.mp4\"\n",
    "# slowfast_feats, slowfast_valid = extract_slowfast_video_features(\n",
    "#     test_video_path,\n",
    "#     num_clips=16,\n",
    "#     clip_duration=2.0,\n",
    "#     target_device=device,\n",
    "#     quiet=False,\n",
    "#  )\n",
    "# print(f\"SlowFast features shape: {slowfast_feats.shape}\")\n",
    "# print(f\"Valid clips extracted: {slowfast_valid}\")\n",
    "# if slowfast_valid > 0:\n",
    "#     print(f\"Sample feature vector L2 norm: {np.linalg.norm(slowfast_feats[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1bc71",
   "metadata": {},
   "source": [
    "## 5. Video utility functions\n",
    "\n",
    "Helpers for extracting and jumping to video segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoState:\n",
    "    \"\"\"Global video state manager\"\"\"\n",
    "    def __init__(self):\n",
    "        self.original_video = None\n",
    "        self.current_video = None\n",
    "        self.is_segment = False\n",
    "        self.segment_info = (0.0, 0.0)\n",
    "        self.temp_files = []\n",
    "        self.is_processing = False\n",
    "        self.video_duration = 0.0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset state and remove temporary files\"\"\"\n",
    "        self.cleanup_temp_files()\n",
    "        self.original_video = None\n",
    "        self.current_video = None\n",
    "        self.is_segment = False\n",
    "        self.segment_info = (0.0, 0.0)\n",
    "        self.is_processing = False\n",
    "        self.video_duration = 0.0\n",
    "    \n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"Delete temporary files created by the UI/process.\"\"\"\n",
    "        for temp_file in self.temp_files:\n",
    "            try:\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.remove(temp_file)\n",
    "            except:\n",
    "                pass\n",
    "        self.temp_files = []\n",
    "\n",
    "# Create global state\n",
    "video_state = VideoState()\n",
    "\n",
    "def get_video_duration(video_path: str) -> float:\n",
    "    \"\"\"Get video duration in seconds using ffprobe.\"\"\"\n",
    "    try:\n",
    "        cmd = [\n",
    "            'ffprobe',\n",
    "            '-v', 'error',\n",
    "            '-show_entries', 'format=duration',\n",
    "            '-of', 'default=noprint_wrappers=1:nokey=1',\n",
    "            video_path\n",
    "        ]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            return float(result.stdout.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "def load_video(video_path):\n",
    "    \"\"\"Load a video file into the demo state and return UI elements.\"\"\"\n",
    "    video_state.reset()\n",
    "    video_state.original_video = video_path\n",
    "    video_state.current_video = video_path\n",
    "    \n",
    "    # Get video duration\n",
    "    video_state.video_duration = get_video_duration(video_path)\n",
    "    \n",
    "    # Get file extension\n",
    "    file_ext = os.path.splitext(video_path)[1].lower()\n",
    "    mime_type_map = {\n",
    "        '.mp4': 'video/mp4',\n",
    "        '.avi': 'video/x-msvideo',\n",
    "        '.mov': 'video/quicktime',\n",
    "        '.mkv': 'video/x-matroska',\n",
    "        '.webm': 'video/webm'\n",
    "    }\n",
    "    mime_type = mime_type_map.get(file_ext, 'video/mp4')\n",
    "    \n",
    "    # Create video HTML\n",
    "    video_html = f\"\"\"\n",
    "    <div style=\\\"width: 100%; max-width: 510px; margin: 0 auto; padding: 5px;\\\">\n",
    "        <video \n",
    "            id=\\\"main-video\\\" \n",
    "            controls \n",
    "            width=\\\"100%\\\" \n",
    "            style=\\\"border-radius: 8px; width: 510px; height: 310px; max-width: 510px; max-height: 310px; object-fit: contain; display: block;\\\"\n",
    "            preload=\\\"metadata\\\"\n",
    "        >\n",
    "            <source src=\\\"file={video_path}\\\" type=\\\"{mime_type}\\\">\n",
    "            Your browser does not support this video format.\n",
    "        </video>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    status_msg = f\"Video '{os.path.basename(video_path)}' loaded successfully!\\n\"\n",
    "    if video_state.video_duration > 0:\n",
    "        status_msg += f\"Video duration: {video_state.video_duration:.1f} seconds\\n\"\n",
    "    status_msg += \"You can start retrieval!\"\n",
    "    \n",
    "    if \"create_result_interface\" in globals():\n",
    "        interface_updates = create_result_interface([])\n",
    "    else:\n",
    "        num_candidates = globals().get(\"TOP_K_RESULTS\", 5)\n",
    "        summary_placeholder = gr.update(value=\"<div class='result-summary'>Waiting for retrieval</div>\")\n",
    "        candidate_placeholders = []\n",
    "        for _ in range(num_candidates):\n",
    "            candidate_placeholders.extend([\n",
    "                gr.update(visible=False),\n",
    "                gr.update(value=\"\"),\n",
    "                gr.update(interactive=False),\n",
    "            ])\n",
    "        interface_updates = [summary_placeholder, *candidate_placeholders]\n",
    "    \n",
    "    return (\n",
    "        video_html,\n",
    "        status_msg,\n",
    "        [],\n",
    "        *interface_updates,\n",
    "    )\n",
    "\n",
    "def simple_video_jump(start_time, end_time):\n",
    "    \"\"\"Jump to a specified time range in the video.\"\"\"\n",
    "    if not video_state.original_video:\n",
    "        return \"Please upload a video first\", \"\"\n",
    "    \n",
    "    if start_time is None or end_time is None:\n",
    "        return \"Invalid time range\", \"\"\n",
    "    \n",
    "    # compute midpoint\n",
    "    middle_time = (start_time + end_time) / 2.0\n",
    "    \n",
    "    file_ext = os.path.splitext(video_state.original_video)[1].lower()\n",
    "    mime_type_map = {\n",
    "        '.mp4': 'video/mp4',\n",
    "        '.avi': 'video/x-msvideo',\n",
    "        '.mov': 'video/quicktime',\n",
    "        '.mkv': 'video/x-matroska',\n",
    "        '.webm': 'video/webm',\n",
    "    }\n",
    "    mime_type = mime_type_map.get(file_ext, 'video/mp4')\n",
    "    \n",
    "    video_html = f\"\"\"\n",
    "    <div style=\\\"width: 100%; max-width: 510px; margin: 0 auto; padding: 5px;\\\">\n",
    "        <video \n",
    "            id=\\\"main_video\\\" \n",
    "            controls \n",
    "            width=\\\"100%\\\" \n",
    "            style=\\\"border-radius: 8px; width: 510px; height: 300px; max-width: 510px; max-height: 300px; object-fit: contain; display: block;\\\"\n",
    "            preload=\\\"metadata\\\"\n",
    "        >\n",
    "            <source src=\\\"file={video_state.original_video}#t={middle_time}\\\" type=\\\"{mime_type}\\\">\n",
    "            Your browser does not support video playback.\n",
    "        </video>\n",
    "        <div style=\\\"background: #f0f8ff; padding: 8px; margin-top: 8px; border-radius: 4px; text-align: center;\\\">\n",
    "            Current segment: {start_time:.1f}s - {end_time:.1f}s\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    status_msg = f\"Jumped to: {start_time:.1f}s - {end_time:.1f}s (middle position: {middle_time:.1f}s)\"\n",
    "    \n",
    "    return status_msg, video_html\n",
    "\n",
    "\n",
    "# def simple_video_jump(start_time, end_time):\n",
    "#     \"\"\"Ë∑≥ËΩ¨Âà∞ËßÜÈ¢ëÁöÑÊåáÂÆöÊó∂Âàª\"\"\"\n",
    "#     if not video_state.original_video:\n",
    "#         return \"ËØ∑ÂÖà‰∏ä‰º†ËßÜÈ¢ë\", \"\"\n",
    "    \n",
    "#     if start_time is None or end_time is None:\n",
    "#         return \"Êó†ÊïàÁöÑÊó∂Èó¥ËåÉÂõ¥\", \"\"\n",
    "    \n",
    "#     file_ext = os.path.splitext(video_state.original_video)[1].lower()\n",
    "#     mime_type_map = {\n",
    "#         '.mp4': 'video/mp4',\n",
    "#         '.avi': 'video/x-msvideo',\n",
    "#         '.mov': 'video/quicktime',\n",
    "#         '.mkv': 'video/x-matroska',\n",
    "#         '.webm': 'video/webm'\n",
    "#     }\n",
    "#     mime_type = mime_type_map.get(file_ext, 'video/mp4')\n",
    "    \n",
    "#     video_html = f\"\"\"\n",
    "#     <div style=\\\"width: 100%; max-width: 510px; margin: 0 auto; padding: 5px;\\\">\n",
    "#         <video \n",
    "#             id=\\\"main_video\\\" \n",
    "#             controls \n",
    "#             width=\\\"100%\\\" \n",
    "#             style=\\\"border-radius: 8px; width: 510px; height: 300px; max-width: 510px; max-height: 300px; object-fit: contain; display: block;\\\"\n",
    "#             preload=\\\"metadata\\\"\n",
    "#         >\n",
    "#             <source src=\\\"file={video_state.original_video}#t={start_time}\\\" type=\\\"{mime_type}\\\">\n",
    "#             ÊÇ®ÁöÑÊµèËßàÂô®‰∏çÊîØÊåÅËßÜÈ¢ëÊí≠Êîæ„ÄÇ\n",
    "#         </video>\n",
    "#         <div style=\\\"background: #f0f8ff; padding: 8px; margin-top: 8px; border-radius: 4px; text-align: center;\\\">\n",
    "#             ÂΩìÂâçÊó∂Èó¥ÊÆµ: {start_time:.1f}s - {end_time:.1f}s\n",
    "#         </div>\n",
    "#     </div>\n",
    "#     \"\"\"\n",
    "    \n",
    "#     status_msg = f\"Ë∑≥ËΩ¨Âà∞: {start_time:.1f}s - {end_time:.1f}s\"\n",
    "    \n",
    "#     return status_msg, video_html\n",
    "\n",
    "# print(\"Video processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21137801",
   "metadata": {},
   "source": [
    "## 6. Model inference function\n",
    "`search_moments` is the main orchestrator for inference:\n",
    "- lazily load a trained Switch-NET checkpoint and cache the model/config;\n",
    "- choose fusion strategy based on `vid_dim`: 514 uses CLIP+TEF, 2818 also includes SlowFast features;\n",
    "- build masks for video/text to match the sequence lengths used at training;\n",
    "- run the model and convert center-width span predictions to start-end times for the frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ab781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search function defined!\n"
     ]
    }
   ],
   "source": [
    "# Global model cache\n",
    "class ModelCache:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.config = None\n",
    "        self.is_loaded = False\n",
    "\n",
    "model_cache = ModelCache()\n",
    "\n",
    "TOP_K_RESULTS = 5\n",
    "\n",
    "def search_moments(query_text, video_player_html, model_path=\"output/models/best_model_moe.pth\"):\n",
    "    \"\"\"\n",
    "    Search for relevant moments in a video using the Switch-NET model.\n",
    "    \n",
    "    Args:\n",
    "        query_text: the query string\n",
    "        video_player_html: the video player HTML (used to get state)\n",
    "        model_path: path to the model checkpoint\n",
    "    \n",
    "    Yields:\n",
    "        (status_message, results) for incremental UI updates; results are ranked candidate lists.\n",
    "    \"\"\"\n",
    "    if not query_text:\n",
    "        gr.Warning(\"Please enter a query!\")\n",
    "        yield \"Please enter a query\", []\n",
    "        return\n",
    "    \n",
    "    actual_video_path = video_state.original_video\n",
    "    if not actual_video_path:\n",
    "        gr.Warning(\"Please upload a video first!\")\n",
    "        yield \"Please upload a video file first\", []\n",
    "        return\n",
    "    \n",
    "    if video_state.is_processing:\n",
    "        gr.Warning(\"Processing is in progress, please wait...\")\n",
    "        yield \"Processing is in progress, please wait...\", []\n",
    "        return\n",
    "    \n",
    "    video_state.is_processing = True\n",
    "\n",
    "    try:\n",
    "        if not model_cache.is_loaded:\n",
    "            yield \"Loading Switch-NET model...\\n\", []\n",
    "\n",
    "            if not os.path.exists(model_path):\n",
    "                yield f\"Error: model file not found {model_path}\\nPlease train the model or provide a correct checkpoint path\", []\n",
    "                return\n",
    "\n",
    "            model_cache.model, model_cache.config = load_trained_model(model_path, device)\n",
    "            model_cache.is_loaded = True\n",
    "            yield \"Model loaded!\\n\", []\n",
    "\n",
    "        cfg = model_cache.config or {}\n",
    "        vid_dim = int(cfg.get(\"vid_dim\", 512))\n",
    "        txt_dim = int(cfg.get(\"txt_dim\", 512))\n",
    "        max_v_l = int(cfg.get(\"max_v_l\", 64))\n",
    "        max_q_l = int(cfg.get(\"max_q_l\", 32))\n",
    "        num_queries = int(cfg.get(\"num_queries\", 10))\n",
    "        clip_duration = float(cfg.get(\"clip_len\", SLOWFAST_CLIP_SECONDS))\n",
    "\n",
    "        cache_key = _make_cache_key(\n",
    "            actual_video_path,\n",
    "            max_v_l,\n",
    "            vid_dim,\n",
    "            clip_duration if vid_dim != 514 else None,\n",
    "        )\n",
    "        cache_entry = get_cached_video_features(cache_key, vid_dim)\n",
    "\n",
    "        fused_video_features: Optional[np.ndarray] = None\n",
    "        fused_valid_len: Optional[int] = None\n",
    "\n",
    "        if cache_entry is not None:\n",
    "            fused_video_features = cache_entry[\"fused_features\"]\n",
    "            fused_valid_len = cache_entry[\"fused_valid_len\"]\n",
    "            yield \"Using cached video features...\\n\", []\n",
    "        else:\n",
    "            yield \"Extracting video features...\\n\", []\n",
    "            clip_feats, clip_valid_len = extract_video_clip_features(\n",
    "                actual_video_path, num_clips=max_v_l, feature_dim=512\n",
    "            )\n",
    "            if clip_valid_len == 0:\n",
    "                yield \"Failed to extract frames from the video. Please ensure a valid video file was uploaded\", []\n",
    "                return\n",
    "\n",
    "            tef_features = build_tef_features(max_v_l)\n",
    "            if tef_features.shape[0] != max_v_l:\n",
    "                tef_features = np.resize(tef_features, (max_v_l, TEF_DIM))\n",
    "\n",
    "            fused_valid_len = clip_valid_len\n",
    "\n",
    "            if vid_dim == 514:\n",
    "                fused_video_features = np.concatenate([clip_feats, tef_features], axis=-1)\n",
    "            elif vid_dim == 2818:\n",
    "                slowfast_feats, slowfast_valid = extract_slowfast_video_features(\n",
    "                    actual_video_path,\n",
    "                    num_clips=max_v_l,\n",
    "                    clip_duration=clip_duration,\n",
    "                    target_device=device,\n",
    "                    quiet=True,\n",
    "                )\n",
    "                slowfast_padded = np.zeros((max_v_l, SLOWFAST_FEATURE_DIM), dtype=np.float32)\n",
    "                if slowfast_valid > 0:\n",
    "                    slowfast_len = min(slowfast_valid, max_v_l)\n",
    "                    slowfast_padded[:slowfast_len] = slowfast_feats[:slowfast_len]\n",
    "                    fused_valid_len = max(fused_valid_len, slowfast_len)\n",
    "                else:\n",
    "                    print(\"Warning: SlowFast feature extraction failed, using zero vectors as placeholder.\")\n",
    "                fused_video_features = np.concatenate([clip_feats, slowfast_padded, tef_features], axis=-1)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"vid_dim={} in model config is not supported by the inference pipeline; expected 514 or 2818\".format(vid_dim)\n",
    "                )\n",
    "\n",
    "            if fused_video_features.shape[1] != vid_dim:\n",
    "                raise ValueError(\n",
    "                    f\"Fused video feature dim is {fused_video_features.shape[1]}, which does not match vid_dim={vid_dim} from config\"\n",
    "                )\n",
    "\n",
    "            fused_valid_len = max(1, min(fused_valid_len, max_v_l))\n",
    "            cache_video_features(cache_key, fused_video_features, fused_valid_len, vid_dim)\n",
    "\n",
    "        fused_video_features = np.ascontiguousarray(fused_video_features)\n",
    "        fused_valid_len = max(1, min(int(fused_valid_len), max_v_l))\n",
    "        video_mask = np.zeros(max_v_l, dtype=np.float32)\n",
    "        video_mask[:fused_valid_len] = 1.0\n",
    "\n",
    "        yield \"Extracting query features...\\n\", []\n",
    "        query_features, query_valid_len = extract_query_clip_features(\n",
    "            query_text, seq_len=max_q_l, feature_dim=txt_dim\n",
    "        )\n",
    "        if query_valid_len == 0:\n",
    "            yield \"Query text is empty or could not be encoded; please re-enter\", []\n",
    "            return\n",
    "        query_length = max(1, min(query_valid_len, max_q_l))\n",
    "        query_mask = np.zeros(max_q_l, dtype=np.float32)\n",
    "        query_mask[:query_length] = 1.0\n",
    "\n",
    "        video_features_tensor = torch.from_numpy(fused_video_features).unsqueeze(0).to(device)\n",
    "        query_features_tensor = torch.from_numpy(query_features).unsqueeze(0).to(device)\n",
    "        video_mask_tensor = torch.from_numpy(video_mask).unsqueeze(0).to(device)\n",
    "        query_mask_tensor = torch.from_numpy(query_mask).unsqueeze(0).to(device)\n",
    "\n",
    "        yield \"Running Switch-NET model inference...\\n\", []\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cache.model(\n",
    "                src_txt=query_features_tensor,\n",
    "                src_txt_mask=query_mask_tensor.bool(),\n",
    "                src_vid=video_features_tensor,\n",
    "                src_vid_mask=video_mask_tensor.bool(),\n",
    "                is_training=False,\n",
    "            )\n",
    "\n",
    "        yield \"Processing model predictions...\\n\", []\n",
    "        pred_logits = outputs[\"pred_logits\"][0]  # (num_queries, 2)\n",
    "        pred_spans = outputs[\"pred_spans\"][0]    # (num_queries, 2) center-width\n",
    "        scores = F.softmax(pred_logits, dim=-1)[:, 1]\n",
    "\n",
    "        spans_start_end = span_cxw_to_xx(pred_spans)\n",
    "        spans_start_end = torch.clamp(spans_start_end, 0.0, 1.0)\n",
    "\n",
    "        top_k = min(TOP_K_RESULTS, scores.shape[0])\n",
    "        topk_scores, topk_indices = torch.topk(scores, k=top_k)\n",
    "\n",
    "        candidate_pool = []\n",
    "        for rank, (score, idx) in enumerate(zip(topk_scores, topk_indices), start=1):\n",
    "            candidate_pool.append({\n",
    "                \"rank\": rank,\n",
    "                \"confidence\": float(score.item()),\n",
    "                \"start_norm\": float(spans_start_end[idx, 0].item()),\n",
    "                \"end_norm\": float(spans_start_end[idx, 1].item()),\n",
    "                \"query_index\": int(idx.item()),\n",
    "            })\n",
    "\n",
    "        video_duration = video_state.video_duration or 100.0\n",
    "\n",
    "        results = []\n",
    "        for candidate in candidate_pool:\n",
    "            start_time = candidate[\"start_norm\"] * video_duration\n",
    "            end_time = candidate[\"end_norm\"] * video_duration\n",
    "            if start_time > end_time:\n",
    "                start_time, end_time = end_time, start_time\n",
    "            results.append({\n",
    "                \"rank\": candidate[\"rank\"],\n",
    "                \"start_time\": float(start_time),\n",
    "                \"end_time\": float(end_time),\n",
    "                \"confidence\": candidate[\"confidence\"],\n",
    "                \"query_index\": candidate[\"query_index\"],\n",
    "            })\n",
    "\n",
    "        if not results:\n",
    "            yield \"No valid candidate results obtained\", []\n",
    "            return\n",
    "\n",
    "        best_result = results[0]\n",
    "\n",
    "        status_msg = [\n",
    "            \"Retrieval complete!\",\n",
    "            f\"Top1 match: {best_result['start_time']:.1f}s - {best_result['end_time']:.1f}s\",\n",
    "            f\"Confidence: {best_result['confidence']:.3f}\",\n",
    "        ]\n",
    "        status_msg = \"\\n\".join(status_msg)\n",
    "\n",
    "        yield status_msg, results\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"An error occurred during processing: {str(e)}\"\n",
    "        yield error_msg, []\n",
    "    finally:\n",
    "        video_state.is_processing = False\n",
    "\n",
    "print(\"Search function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f87696",
   "metadata": {},
   "source": [
    "## 7. Gradio UI\n",
    "\n",
    "Build the interactive web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a6a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio interface created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from output/models/best_model_moe.pth\n",
      "Resolved config: {'txt_dim': 512, 'vid_dim': 2818, 'hidden_dim': 256, 'num_queries': 5, 'aux_loss': True, 'position_embedding': 'sine', 'max_v_l': 194, 'max_q_l': 32, 'span_loss_type': 'l1', 'use_txt_pos': False, 'aud_dim': 0, 'queue_length': 65536, 'momentum': 0.995, 'distillation_coefficient': 0.3, 'num_v2t_encoder_layers': 2, 'num_encoder1_layers': 2, 'num_convolutional_blocks': 4, 'num_encoder2_layers': 2, 'num_decoder_layers': 2, 'num_decoder_loops': 3, 'clip_len': 1}\n",
      "Trained for 20 epochs\n",
      "Loaded CLIP model (ViT-B/32) on cpu\n"
     ]
    }
   ],
   "source": [
    "def create_result_interface(results):\n",
    "    \"\"\"Update UI components based on candidate results.\"\"\"\n",
    "    if results:\n",
    "        best = results[0]\n",
    "        summary_html = (\n",
    "            \"<div class='result-summary'>\"\n",
    "            \"<div class='summary-title'>Most relevant time segment</div>\"\n",
    "            f\"<div class='summary-time'>{best['start_time']:.1f}s - {best['end_time']:.1f}s</div>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "    else:\n",
    "        summary_html = (\n",
    "            \"<div class='result-summary result-summary--empty'>\"\n",
    "            \"Waiting for results\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    updates = [gr.update(value=summary_html)]\n",
    "\n",
    "    for idx in range(TOP_K_RESULTS):\n",
    "        if idx < len(results):\n",
    "            candidate = results[idx]\n",
    "            info_html = (\n",
    "                \"<div class='candidate-info'>\"\n",
    "                f\"<span class='candidate-rank'>Candidate {candidate['rank']}</span>\"\n",
    "                f\"<span class='candidate-time'>{candidate['start_time']:.1f}s - {candidate['end_time']:.1f}s</span>\"\n",
    "                \"</div>\"\n",
    "            )\n",
    "            updates.extend([\n",
    "                gr.update(visible=True),\n",
    "                gr.update(value=info_html),\n",
    "                gr.update(interactive=True),\n",
    "            ])\n",
    "        else:\n",
    "            updates.extend([\n",
    "                gr.update(visible=False),\n",
    "                gr.update(value=\"\"),\n",
    "                gr.update(interactive=False),\n",
    "            ])\n",
    "\n",
    "    return updates\n",
    "\n",
    "CSS = \"\"\"\n",
    "#video_player_container {\n",
    "    width: 100% !important;\n",
    "    max-width: 600px !important;\n",
    "    height: 400px !important;\n",
    "    margin: 0 auto 15px auto !important;\n",
    "    display: flex !important;\n",
    "    align-items: center !important;\n",
    "    justify-content: center !important;\n",
    "    border: 2px dashed #e0e0e0 !important;\n",
    "    border-radius: 8px !important;\n",
    "    background-color: #fafafa !important;\n",
    "}\n",
    "\n",
    ".result-summary {\n",
    "    background: #f7fbff !important;\n",
    "    border: 1px solid #d3e3ff !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 12px 16px !important;\n",
    "    margin-bottom: 12px !important;\n",
    "    display: flex !important;\n",
    "    flex-direction: column !important;\n",
    "    gap: 6px !important;\n",
    "}\n",
    "\n",
    ".result-summary--empty {\n",
    "    background: #f9f9f9 !important;\n",
    "    border-color: #e5e5e5 !important;\n",
    "    color: #7a7a7a !important;\n",
    "}\n",
    "\n",
    ".summary-title {\n",
    "    font-weight: 600 !important;\n",
    "    font-size: 15px !important;\n",
    "    color: #3056d3 !important;\n",
    "}\n",
    "\n",
    ".summary-time {\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 700 !important;\n",
    "    color: #1f2937 !important;\n",
    "}\n",
    "\n",
    ".candidate-row {\n",
    "    align-items: center !important;\n",
    "    margin-bottom: 8px !important;\n",
    "}\n",
    "\n",
    ".candidate-info {\n",
    "    background: #ffffff !important;\n",
    "    border: 1px solid #e5e7eb !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 10px 14px !important;\n",
    "    flex: 1 !important;\n",
    "    display: flex !important;\n",
    "    justify-content: space-between !important;\n",
    "    align-items: center !important;\n",
    "    font-size: 14px !important;\n",
    "    color: #1f2937 !important;\n",
    "}\n",
    "\n",
    ".candidate-rank {\n",
    "    font-weight: 600 !important;\n",
    "    color: #2563eb !important;\n",
    "}\n",
    "\n",
    ".candidate-time {\n",
    "    font-variant-numeric: tabular-nums !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=CSS) as demo:\n",
    "    gr.Markdown(\"# üé¨ Switch-NET Video Moment Retrieval\")\n",
    "    gr.Markdown(\"Upload a video, enter a natural language query, and use the Switch-NET model to find relevant moments in the video!\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            video_player = gr.HTML(\n",
    "                value='<div style=\"width: 400px; height: 200px; border: 2px dashed #ccc; border-radius: 8px; display: flex; align-items: center; justify-content: center; background-color: #f9f9f9;\"><div style=\"text-align: center; color: #666;\"><h3>üìπ Video Player</h3><p>Please upload a video file to begin</p></div></div>',\n",
    "                elem_id=\"video_player_container\",\n",
    "            )\n",
    "            upload_button = gr.UploadButton(\"Upload video file\", file_types=[\"video\"])\n",
    "            status_display = gr.Textbox(label=\"Status\", interactive=False, lines=8)\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            query_box = gr.Textbox(label=\"Query\", placeholder=\"For example: a person is chopping peppers...\")\n",
    "            search_button = gr.Button(\"üîç Search\", variant=\"primary\")\n",
    "\n",
    "            gr.Markdown(\"### Search Results\")\n",
    "            summary_html = gr.HTML(\"<div class='result-summary result-summary--empty'>Waiting for retrieval</div>\")\n",
    "            gr.Markdown(\"#### Candidate time segments list\")\n",
    "\n",
    "            candidate_components = []\n",
    "            for idx in range(TOP_K_RESULTS):\n",
    "                with gr.Row(visible=False, elem_classes=\"candidate-row\") as row:\n",
    "                    info = gr.HTML(\"\")\n",
    "                    btn = gr.Button(\"Go\", variant=\"secondary\")\n",
    "                candidate_components.append((row, info, btn))\n",
    "\n",
    "    search_results = gr.State([])\n",
    "\n",
    "    upload_outputs = [\n",
    "        video_player,\n",
    "        status_display,\n",
    "        search_results,\n",
    "        summary_html,\n",
    "    ]\n",
    "    for row, info, btn in candidate_components:\n",
    "        upload_outputs.extend([row, info, btn])\n",
    "\n",
    "    upload_button.upload(\n",
    "        fn=load_video,\n",
    "        inputs=[upload_button],\n",
    "        outputs=upload_outputs,\n",
    "    )\n",
    "\n",
    "    def handle_search_and_update(query_text, video_path):\n",
    "        for status, results in search_moments(query_text, video_path):\n",
    "            interface_updates = create_result_interface(results)\n",
    "            yield status, results, *interface_updates\n",
    "\n",
    "    search_outputs = [\n",
    "        status_display,\n",
    "        search_results,\n",
    "        summary_html,\n",
    "    ]\n",
    "    for row, info, btn in candidate_components:\n",
    "        search_outputs.extend([row, info, btn])\n",
    "\n",
    "    search_button.click(\n",
    "        fn=handle_search_and_update,\n",
    "        inputs=[query_box, video_player],\n",
    "        outputs=search_outputs,\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "    def jump_to_candidate(results, candidate_index):\n",
    "        if video_state.is_processing:\n",
    "            return \"Processing is in progress, please wait...\", gr.update()\n",
    "        if not results:\n",
    "            return \"No available results\", gr.update()\n",
    "        if candidate_index >= len(results):\n",
    "            return \"Candidate index out of range\", gr.update()\n",
    "        candidate = results[candidate_index]\n",
    "        status, video_html = simple_video_jump(candidate['start_time'], candidate['end_time'])\n",
    "        return status, video_html\n",
    "\n",
    "    for idx, (_, _, btn) in enumerate(candidate_components):\n",
    "        btn.click(\n",
    "            fn=jump_to_candidate,\n",
    "            inputs=[search_results, gr.State(idx)],\n",
    "            outputs=[status_display, video_player]\n",
    "        )\n",
    "\n",
    "print(\"Gradio interface created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702147a",
   "metadata": {},
   "source": [
    "## 8. Launch the app\n",
    "\n",
    "Run the Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad23b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the demo\n",
    "demo.launch(debug=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
