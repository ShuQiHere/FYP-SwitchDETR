{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70efac55",
   "metadata": {},
   "source": [
    "# Switch-Net: Expert Iterative Decoder Network for Video Moment Retrieval\n",
    "\n",
    "This notebook implements Switch-Net, a state-of-the-art model for video moment retrieval and highlight detection. Switch-Net addresses three key challenges in video-language understanding:\n",
    "\n",
    "1. **Overlapping semantic information** in contrastive learning → Solved by Distill Align module\n",
    "2. **Inefficient local video feature extraction** → Solved by Convolutional Fuser\n",
    "3. **Inadequate multimodal decoding** → Solved by Loop Decoder with iterative refinement\n",
    "\n",
    "**Architecture Overview:** Video/Text Encoders → Distill Align → Convolutional Fuser → Loop Decoder → Prediction Heads\n",
    "\n",
    "**Paper:** https://arxiv.org/abs/2501.10787"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaedcb",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "Import core libraries for deep learning (PyTorch), numerical computation (NumPy), and data processing. Set random seeds for reproducibility across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a08962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.ops as ops\n",
    "import wandb\n",
    "\n",
    "# Import required for Hungarian algorithm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torchvision\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b8b60",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Core utilities for span manipulation, IoU computation, and accuracy metrics:\n",
    "- **Span conversion:** Transform between (center, width) and (start, end) formats for different loss functions\n",
    "- **Generalized IoU:** Compute temporal intersection-over-union with penalty for non-overlapping regions\n",
    "- **Position encoding:** Generate sinusoidal embeddings for temporal positions\n",
    "- **Accuracy:** Compute top-k classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667473cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_xx_to_cxw(xx_spans: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Convert [start, end] spans to [center, width].\"\"\"\n",
    "    center = xx_spans.sum(-1) * 0.5\n",
    "    width = xx_spans[..., 1] - xx_spans[..., 0]\n",
    "    return torch.stack([center, width], dim=-1)\n",
    "\n",
    "def span_cxw_to_xx(cxw_spans: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Convert [center, width] spans to [start, end].\"\"\"\n",
    "    x1 = cxw_spans[..., 0] - 0.5 * cxw_spans[..., 1]\n",
    "    x2 = cxw_spans[..., 0] + 0.5 * cxw_spans[..., 1]\n",
    "    return torch.stack([x1, x2], dim=-1)\n",
    "\n",
    "def temporal_iou(spans1: torch.Tensor, spans2: torch.Tensor):\n",
    "    \"\"\"Compute IoU and union between two span sets.\"\"\"\n",
    "    spans1 = spans1.float()\n",
    "    spans2 = spans2.float()\n",
    "    areas1 = spans1[:, 1] - spans1[:, 0]\n",
    "    areas2 = spans2[:, 1] - spans2[:, 0]\n",
    "    left = torch.max(spans1[:, None, 0], spans2[:, 0])\n",
    "    right = torch.min(spans1[:, None, 1], spans2[:, 1])\n",
    "    inter = (right - left).clamp(min=0)\n",
    "    union = areas1[:, None] + areas2 - inter\n",
    "    iou = inter / union.clamp(min=1e-6)\n",
    "    return iou, union\n",
    "\n",
    "def generalized_temporal_iou(spans1: torch.Tensor, spans2: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Generalized IoU for 1D spans.\"\"\"\n",
    "    spans1 = spans1.float()\n",
    "    spans2 = spans2.float()\n",
    "    assert (spans1[:, 1] >= spans1[:, 0]).all()\n",
    "    assert (spans2[:, 1] >= spans2[:, 0]).all()\n",
    "    iou, union = temporal_iou(spans1, spans2)\n",
    "    left = torch.min(spans1[:, None, 0], spans2[:, 0])\n",
    "    right = torch.max(spans1[:, None, 1], spans2[:, 1])\n",
    "    enclosing = (right - left).clamp(min=0)\n",
    "    return iou - (enclosing - union) / enclosing.clamp(min=1e-6)\n",
    "\n",
    "def get_detr_position_encoding(max_length, num_pos_feats=128):\n",
    "    \"\"\"\n",
    "    Create positional encodings similar to DETR for video sequences.\n",
    "    Used when raw counting positions are needed.\n",
    "    \"\"\"\n",
    "    position = torch.arange(max_length, dtype=torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, num_pos_feats, 2, dtype=torch.float32) * (-math.log(10000.0) / num_pos_feats))\n",
    "    pe = torch.zeros(max_length, num_pos_feats)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "def gen_sineembed_for_position(pos_tensor):\n",
    "    \"\"\"\n",
    "    Generate sine/cosine embeddings for reference points.\n",
    "    Used when normalized [0, 1] positions are needed.\n",
    "    \"\"\"\n",
    "    scale = 2 * math.pi\n",
    "    dim_t = torch.arange(128, dtype=torch.float32, device=pos_tensor.device)\n",
    "    dim_t = 10000 ** (2 * (dim_t // 2) / 128)\n",
    "    center_embed = pos_tensor[:, :, 0] * scale\n",
    "    pos_x = center_embed[:, :, None] / dim_t\n",
    "    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    span_embed = pos_tensor[:, :, 1] * scale\n",
    "    pos_w = span_embed[:, :, None] / dim_t\n",
    "    pos_w = torch.stack((pos_w[:, :, 0::2].sin(), pos_w[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    return torch.cat((pos_x, pos_w), dim=2)\n",
    "\n",
    "def inverse_sigmoid(x, eps=1e-3):\n",
    "    \"\"\"Inverse sigmoid function\"\"\"\n",
    "    x = x.clamp(min=0, max=1)\n",
    "    x1 = x.clamp(min=eps)\n",
    "    x2 = (1 - x).clamp(min=eps)\n",
    "    return torch.log(x1 / x2)\n",
    "\n",
    "# Accuracy calculation\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Compute top-k accuracy (matches Switch-Net's (formerly LD-DETR) implementation).\"\"\"\n",
    "    maxk = max(topk)\n",
    "    num_items = output.size(0)\n",
    "    if num_items == 0:\n",
    "        return [output.new_tensor(0.0) for _ in topk]\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "\n",
    "    if isinstance(target, torch.Tensor):\n",
    "        if target.numel() == 0:\n",
    "            return [output.new_tensor(0.0) for _ in topk]\n",
    "        target = target.view(1, -1).expand_as(pred)\n",
    "    else:\n",
    "        target = torch.full_like(pred, target)\n",
    "\n",
    "    correct = pred.eq(target)\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / max(num_items, 1)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b50c24",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Inject temporal information into feature sequences using sinusoidal functions at different frequencies. This allows the model to understand the relative ordering of video frames and text tokens.\n",
    "\n",
    "**Two implementations:**\n",
    "- **Sine:** Continuous sinusoidal positional encoding with learnable normalization scale\n",
    "- **Learned:** Trainable position embeddings (similar to BERT)\n",
    "\n",
    "**Why needed:** Transformers are permutation-invariant by design. Positional encoding provides crucial sequence order information for temporal understanding.\n",
    "\n",
    "**Output dimension:** Returns `hidden_dim` dimensional embeddings (duplicates `num_pos_feats` to match model dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    Sine position embedding for video features\n",
    "    This is a learnable positional encoding based on sine/cosine functions\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=128, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, L, d)\n",
    "            mask: (batch_size, L), with 1 indicating valid positions\n",
    "        Returns:\n",
    "            pos: (batch_size, L, d) positional encoding\n",
    "        \"\"\"\n",
    "        assert mask is not None, \"mask is required for positional encoding\"\n",
    "        \n",
    "        # Mask uses 1.0 for valid tokens; accumulate positions across valid entries.\n",
    "        valid = mask.to(dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Compute cumulative sum of valid positions, e.g., [1,1,1,0,0] -> [1,2,3,3,3]\n",
    "        x_embed = valid.cumsum(1, dtype=torch.float32)\n",
    "\n",
    "        # If normalization is enabled, scale positions to [0, scale], otherwise positions are raw counts\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            lengths = x_embed[:, -1:].clamp(min=eps)\n",
    "            x_embed = x_embed / lengths * self.scale\n",
    "\n",
    "        # Zero out positions corresponding to invalid tokens, e.g., [1,2,3,3,3] -> [1,2,3,0,0]\n",
    "        x_embed = x_embed * valid\n",
    "\n",
    "        # Create a 1D tensor of dimension indices: [0, 1, 2, ..., num_pos_feats-1]\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        \n",
    "        # 10000^(2i / d_model) (denominator of the positional encoding formula)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        # pos_x = pos/dim_t\n",
    "        pos_x = x_embed[:, :, None] / dim_t  # (batch_size, L, num_pos_feats)\n",
    "\n",
    "        # concatenate sin and cos embeddings\n",
    "        pos = torch.cat((pos_x.sin(), pos_x.cos()), dim=-1)\n",
    "        return pos\n",
    "\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"Learned positional embeddings.\"\"\"\n",
    "    def __init__(self, num_pos_feats=256, max_len=512):\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_len, num_pos_feats)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        return self.position_embeddings(position_ids)\n",
    "\n",
    "def build_position_encoding(hidden_dim, position_embedding_type=\"sine\", max_len=512):\n",
    "    \"\"\"Factory that returns video/text positional encoders.\"\"\"\n",
    "    n_steps = hidden_dim // 2\n",
    "    if position_embedding_type == \"sine\":\n",
    "        position_embed = PositionEmbeddingSine(n_steps, normalize=True)\n",
    "        txt_position_embed = PositionEmbeddingSine(n_steps, normalize=True)\n",
    "    elif position_embedding_type == \"learned\":\n",
    "        position_embed = PositionEmbeddingLearned(hidden_dim, max_len)\n",
    "        txt_position_embed = PositionEmbeddingLearned(hidden_dim, max_len)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown position embedding type: {position_embedding_type}\")\n",
    "    return position_embed, txt_position_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d793a",
   "metadata": {},
   "source": [
    "## Unimodal Encoder\n",
    "\n",
    "Projects video and text features from their original feature spaces (e.g., CLIP: 512-dim, SlowFast: 2304-dim) into a shared latent space of `hidden_dim=256`. This dimensionality reduction enables efficient cross-modal interaction in subsequent modules.\n",
    "\n",
    "**Architecture:** LayerNorm → Dropout → Linear → ReLU → LayerNorm → Dropout → Linear  \n",
    "**Design rationale:**  \n",
    "- Dual normalization layers stabilize training with pre-extracted features\n",
    "- Heavy dropout (0.5) prevents overfitting to specific feature extractors\n",
    "- ReLU activation introduces non-linearity for better expressiveness\n",
    "\n",
    "**Role in pipeline:** First step in creating unified multimodal representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnimodalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Unimodal Encoder that maps input features to a shared latent space\n",
    "    Architecture: LayerNorm -> Linear -> ReLU -> LayerNorm -> Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            x: encoded features (batch_size, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = self.linear1(self.dropout1(self.norm1(x)))\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.linear2(self.dropout2(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "print(\"Unimodal Encoder loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c4379",
   "metadata": {},
   "source": [
    "## Distill Align Module\n",
    "\n",
    "**Key innovation:** Addresses the fundamental problem of overlapping semantics in video-text contrastive learning.\n",
    "\n",
    "**Problem statement:** Traditional contrastive learning (e.g., CLIP) treats all negatives equally. However, video-text pairs often share partial semantic information:\n",
    "- \"A person walking in the park\" vs \"A person running in the park\"\n",
    "- Both should be partially similar, not completely negative\n",
    "\n",
    "**Solution - Similarity Matrix Distillation:**\n",
    "1. **Momentum encoders:** Maintain stable teacher representations (EMA with β=0.995)\n",
    "2. **Queue mechanism:** Store 65,536 historical features for large-scale negative sampling\n",
    "3. **Distillation loss:** Guide student similarity matrix toward teacher's identity matrix\n",
    "4. **Temperature scaling:** Control the sharpness of similarity distributions\n",
    "\n",
    "**Two complementary losses:**\n",
    "- `loss_align`: Video-text alignment distilled from momentum encoder (coefficient α=0.4)\n",
    "- `loss_sim`: Cross-modal similarity with queue-based hard negatives\n",
    "\n",
    "**Why momentum:** Prevents representation collapse and provides consistent supervision signals during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillAlign(nn.Module):\n",
    "    \"\"\"\n",
    "    Distill Align Module for multimodal alignment\n",
    "    Key innovation: Distills similarity matrices into identity matrices to reduce \n",
    "    the impact of overlapping semantic information\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, queue_length=65536, temp=0.07, alpha=0.4):\n",
    "        super().__init__()\n",
    "        self.temp = nn.Parameter(torch.ones([]) * temp)\n",
    "        self.alpha = alpha\n",
    "        self.queue_length = queue_length\n",
    "        \n",
    "        # Cosine similarity\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "        # Prediction MLP\n",
    "        self.h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Register global feature queues and normalize them\n",
    "        # Initialize queues with random normalized vectors\n",
    "        self.register_buffer(\"vid_queue\", torch.randn(hidden_dim, queue_length))\n",
    "        self.register_buffer(\"txt_queue\", torch.randn(hidden_dim, queue_length))\n",
    "        self.vid_queue = F.normalize(self.vid_queue, dim=0)\n",
    "        self.txt_queue = F.normalize(self.txt_queue, dim=0)\n",
    "        \n",
    "        # Register the pointer\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "        \n",
    "        # Initialize parameters\n",
    "        for _, p in self.h.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src_vid_cls, src_txt_cls, src_vid_cls_m, src_txt_cls_m, \n",
    "                epoch_i, batch_idx, train_loader_length=0, is_training=False):\n",
    "        \"\"\"Distills momentum similarities into the student branch.\n",
    "\n",
    "        Args:\n",
    "            src_vid_cls: video features from student branch (batch_size, hidden_dim)\n",
    "            src_txt_cls: text features from student branch (batch_size, hidden_dim)\n",
    "            src_vid_cls_m: video features from momentum branch (batch_size, hidden_dim)\n",
    "            src_txt_cls_m: text features from momentum branch (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Gradually increase alpha during the first epoch （alpha is the distillation weight）\n",
    "        if epoch_i > 0 or train_loader_length == 0:\n",
    "            alpha = self.alpha\n",
    "        else:\n",
    "            alpha = self.alpha * min(1, batch_idx / train_loader_length)\n",
    "\n",
    "\n",
    "        # Compute Loss_align (\"Learn from teacher\")\n",
    "        with torch.no_grad():\n",
    "            self.temp.clamp_(0.001, 0.5)\n",
    "\n",
    "            # Construct features for similarity computation\n",
    "            # Video Features: [256, B + K]; Text Features: [256, B + K]\n",
    "            vid_feat = torch.cat([src_vid_cls_m.t(), self.vid_queue.clone().detach()], dim=1)\n",
    "            txt_feat = torch.cat([src_txt_cls_m.t(), self.txt_queue.clone().detach()], dim=1)\n",
    "\n",
    "            # Compute similarity matrices from momentum branch\n",
    "            # [B, 256] @ [256, B + K] -> [B, B + K]\n",
    "            sim_v2t_m = src_vid_cls_m @ txt_feat / self.temp\n",
    "            sim_t2v_m = src_txt_cls_m @ vid_feat / self.temp\n",
    "\n",
    "            # Hard label, we believe video_1 is only related to text_1\n",
    "            sim_targets = torch.zeros(sim_v2t_m.size(), device=src_vid_cls.device)\n",
    "            sim_targets.fill_diagonal_(1)\n",
    "\n",
    "            sim_v2t_targets = alpha * F.softmax(sim_v2t_m, dim=1) + (1 - alpha) * sim_targets\n",
    "            sim_t2v_targets = alpha * F.softmax(sim_t2v_m, dim=1) + (1 - alpha) * sim_targets\n",
    "\n",
    "        sim_v2t = src_vid_cls @ txt_feat / self.temp\n",
    "        sim_t2v = src_txt_cls @ vid_feat / self.temp\n",
    "        loss_v2t = -torch.sum(F.log_softmax(sim_v2t, dim=1) * sim_v2t_targets, dim=1).mean()\n",
    "        loss_t2v = -torch.sum(F.log_softmax(sim_t2v, dim=1) * sim_t2v_targets, dim=1).mean()\n",
    "        loss_align = (loss_v2t + loss_t2v) / 2\n",
    "        \n",
    "        if is_training:\n",
    "            self._dequeue_and_enqueue(src_vid_cls_m, src_txt_cls_m)\n",
    "        \n",
    "        p_vid_cls = self.h(F.relu(src_vid_cls, inplace=False))\n",
    "        p_txt_cls = self.h(F.relu(src_txt_cls, inplace=False))\n",
    "        loss_sim = -(self.cos(p_vid_cls, src_txt_cls).mean() + self.cos(p_txt_cls, src_vid_cls).mean()) / 2\n",
    "        return loss_align, loss_sim\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, vid_feat, txt_feat):\n",
    "        \"\"\"Push momentum global features into queues with wrap-around support.\"\"\"\n",
    "        batch_size = vid_feat.shape[0]\n",
    "        if batch_size == 0:\n",
    "            return\n",
    "        vid_feat = vid_feat.detach()\n",
    "        txt_feat = txt_feat.detach()\n",
    "        queue_len = self.queue_length\n",
    "        ptr = int(self.queue_ptr.item())\n",
    "        \n",
    "        if batch_size >= queue_len:\n",
    "            self.vid_queue.copy_(vid_feat[-queue_len:].t())\n",
    "            self.txt_queue.copy_(txt_feat[-queue_len:].t())\n",
    "            ptr = 0\n",
    "        else:\n",
    "            end_ptr = ptr + batch_size\n",
    "            if end_ptr <= queue_len:\n",
    "                self.vid_queue[:, ptr:end_ptr] = vid_feat.t()\n",
    "                self.txt_queue[:, ptr:end_ptr] = txt_feat.t()\n",
    "            else:\n",
    "                first_part = queue_len - ptr\n",
    "                if first_part > 0:\n",
    "                    self.vid_queue[:, ptr:] = vid_feat[:first_part].t()\n",
    "                    self.txt_queue[:, ptr:] = txt_feat[:first_part].t()\n",
    "                overflow = end_ptr - queue_len\n",
    "                if overflow > 0:\n",
    "                    self.vid_queue[:, :overflow] = vid_feat[first_part:].t()\n",
    "                    self.txt_queue[:, :overflow] = txt_feat[first_part:].t()\n",
    "            ptr = (ptr + batch_size) % queue_len\n",
    "        self.queue_ptr[0] = ptr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ccfbd",
   "metadata": {},
   "source": [
    "## Transformer Components\n",
    "\n",
    "Standard Transformer encoder and decoder layers forming the backbone for multimodal fusion and temporal modeling.\n",
    "\n",
    "**TransformerEncoderLayer:**\n",
    "- Self-attention mechanism for modeling within-modality relationships\n",
    "- Position-aware: Queries and keys enhanced with positional encoding\n",
    "- FFN (Feed-Forward Network): Two-layer MLP with ReLU/GELU activation\n",
    "- Residual connections + Layer normalization for stable gradient flow\n",
    "\n",
    "**TransformerDecoderLayer:**\n",
    "- Self-attention: Models interactions among moment queries\n",
    "- Cross-attention: Attends to fused video-text memory  \n",
    "- Query position handling: Optional position projection for iterative refinement\n",
    "- Three sub-layers: Self-attn → Cross-attn → FFN (each with residual + norm)\n",
    "\n",
    "**Design choices:**\n",
    "- `batch_first=False`: Sequence length as first dimension for compatibility\n",
    "- Flexible activation: ReLU (faster) or GELU (smoother gradients)\n",
    "- `normalize_before`: Optional pre-norm architecture (used in ViT-style models)\n",
    "\n",
    "**Role in Switch-Net:** These form the building blocks for V2T extraction, convolutional fusion, and loop decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "# Top-2 Mixture-of-Experts FFN used inside the decoder\n",
    "class Top2MoEFFN(nn.Module):\n",
    "    \"\"\"Top-2 gated mixture-of-experts feed-forward module.\"\"\"\n",
    "    def __init__(self, d_model, dim_feedforward, num_experts=8, top_k=2,\n",
    "                 dropout=0.1, activation=F.relu, load_balance_coef=1.0):\n",
    "        super().__init__()\n",
    "        assert top_k <= num_experts, \"top_k must be <= num_experts\"\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.activation = activation\n",
    "        self.load_balance_coef = load_balance_coef\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "        self.expert_fc1 = nn.ModuleList(\n",
    "            nn.Linear(d_model, dim_feedforward) for _ in range(num_experts))\n",
    "        self.expert_fc2 = nn.ModuleList(\n",
    "            nn.Linear(dim_feedforward, d_model) for _ in range(num_experts))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            output: tensor with same shape as x after expert routing\n",
    "            load_balance_loss: scalar auxiliary loss encouraging balanced expert usage\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = x.shape\n",
    "        token_count = seq_len * batch_size\n",
    "        if token_count == 0:\n",
    "            return x, x.new_zeros(())\n",
    "\n",
    "        x_flat = x.reshape(token_count, -1)\n",
    "        router_logits = self.gate(x_flat)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "\n",
    "        topk_probs, topk_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "        topk_probs = topk_probs / (topk_probs.sum(dim=-1, keepdim=True) + 1e-9)\n",
    "\n",
    "        dispatch_mask = F.one_hot(topk_indices, num_classes=self.num_experts).to(router_probs.dtype)\n",
    "        load_denominator = float(max(token_count * self.top_k, 1))\n",
    "        expert_load = dispatch_mask.sum(dim=(0, 1)) / load_denominator\n",
    "        router_prob_mean = router_probs.mean(dim=0)\n",
    "        load_balance_loss = router_prob_mean * expert_load\n",
    "        load_balance_loss = load_balance_loss.sum() * self.num_experts * self.load_balance_coef\n",
    "\n",
    "        final_output_flat = x_flat.new_zeros(x_flat.shape)\n",
    "\n",
    "        for expert_id in range(self.num_experts):\n",
    "            fc1 = self.expert_fc1[expert_id]\n",
    "            fc2 = self.expert_fc2[expert_id]\n",
    "            for slot_idx in range(self.top_k):\n",
    "                slot_mask = topk_indices[:, slot_idx] == expert_id\n",
    "                if not slot_mask.any():\n",
    "                    continue\n",
    "                token_indices = torch.nonzero(slot_mask, as_tuple=False).squeeze(1)\n",
    "                expert_input = x_flat.index_select(0, token_indices)\n",
    "                gate_weight = topk_probs.index_select(0, token_indices)[:, slot_idx]\n",
    "                hidden = fc1(expert_input)\n",
    "                hidden = self.activation(hidden)\n",
    "                hidden = self.dropout(hidden)\n",
    "                expert_output = fc2(hidden)\n",
    "                weighted_output = expert_output * gate_weight.unsqueeze(1)\n",
    "                final_output_flat.index_add_(0, token_indices, weighted_output)\n",
    "\n",
    "        output = final_output_flat.view(seq_len, batch_size, -1)\n",
    "        return output, load_balance_loss\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Standard Transformer encoder layer\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, \n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    # This implements a standard self-attention + feedforward layer\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (seq_len, batch_size, d_model)\n",
    "            pos: (seq_len, batch_size, d_model) positional encoding\n",
    "        \"\"\"\n",
    "        q = k = src if pos is None else src + pos\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                             key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "# Transformer Decoder Layer\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"Standard Transformer decoder layer with cross-attention\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False, keep_query_pos=False,\n",
    "                 moe_num_experts=8, moe_top_k=2, moe_load_balance_coef=0.01):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.activation_fn = F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation_fn = F.gelu\n",
    "        elif activation == \"prelu\":\n",
    "            self.activation_fn = nn.PReLU()\n",
    "        else:\n",
    "            self.activation_fn = F.gelu\n",
    "        self.activation = self.activation_fn\n",
    "        self.normalize_before = normalize_before\n",
    "        self.keep_query_pos = keep_query_pos\n",
    "        \n",
    "        self.moe_ffn = Top2MoEFFN(\n",
    "            d_model,\n",
    "            dim_feedforward,\n",
    "            num_experts=moe_num_experts,\n",
    "            top_k=moe_top_k,\n",
    "            dropout=dropout,\n",
    "            activation=self.activation_fn,\n",
    "            load_balance_coef=moe_load_balance_coef,\n",
    "        )\n",
    "        \n",
    "        # For query position\n",
    "        self.ca_qpos_proj = nn.Linear(d_model, d_model) if keep_query_pos else None\n",
    "\n",
    "    # This implements self-attention, cross-attention, and feedforward\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None,\n",
    "                pos=None, query_pos=None, query_sine_embed=None, is_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: (num_queries, batch_size, d_model)\n",
    "            memory: (seq_len, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention\n",
    "        q = k = tgt if query_pos is None else tgt + query_pos\n",
    "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
    "                             key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention\n",
    "        if self.ca_qpos_proj is not None:\n",
    "            q = tgt + self.ca_qpos_proj(query_pos)\n",
    "        else:\n",
    "            q = tgt + query_pos if query_pos is not None else tgt\n",
    "        \n",
    "        k = memory if pos is None else memory + pos\n",
    "        tgt2 = self.multihead_attn(query=q, key=k, value=memory,\n",
    "                                   attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # Mixture-of-experts feedforward\n",
    "        tgt2, moe_loss = self.moe_ffn(tgt)\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        \n",
    "        return tgt, moe_loss\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    \"\"\"Clone a module N times\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "print(\"Transformer components loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6c448",
   "metadata": {},
   "source": [
    "### 4.4 Convolutional Fuser\n",
    "\n",
    "The next three cells unpack the multimodal fusion stack in detail:\n",
    "\n",
    "- **V2TExtractor** strips away text-irrelevant video content via cross-modal attention so the downstream encoder receives query-aware clips.\n",
    "- **ConvolutionalBlock** alternates temporal convolutions and residual connections to capture local dynamics that transformers might miss.\n",
    "- **ConvolutionalFuser** stitches the pieces together, routing video/text through stacked transformer encoders and the convolutional block before handing a fused memory tensor to the decoder.\n",
    "\n",
    "Skimming the docstrings gives implementation specifics, while the summary above clarifies how the pieces cooperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2T Extractor - removes text-irrelevant information from video features\n",
    "class V2TExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    V2T Extractor extracts text-irrelevant video features\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Weight for context\n",
    "        # Weight for query\n",
    "        # Weight for interaction\n",
    "        w4C = torch.empty(hidden_dim, 1)\n",
    "        w4Q = torch.empty(hidden_dim, 1)\n",
    "        w4mlu = torch.empty(1, 1, hidden_dim)\n",
    "        self.w4C = nn.Parameter(w4C, requires_grad=True)\n",
    "        self.w4Q = nn.Parameter(w4Q, requires_grad=True)\n",
    "        self.w4mlu = nn.Parameter(w4mlu, requires_grad=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.cqa_linear = nn.Conv1d(\n",
    "            in_channels=4 * hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            bias=True,\n",
    "        )\n",
    "        weight = torch.empty(hidden_dim, 1)\n",
    "        self.weight = nn.Parameter(weight, requires_grad=True)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=2 * hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            bias=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize parameters\n",
    "        nn.init.xavier_uniform_(self.w4C)\n",
    "        nn.init.xavier_uniform_(self.w4Q)\n",
    "        nn.init.xavier_uniform_(self.w4mlu)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, src_vid, src_txt, src_vid_mask, src_txt_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vid: video features (batch_size, L, hidden_dim)\n",
    "            src_txt: text features (batch_size, N, hidden_dim)\n",
    "            src_vid_mask: video mask (batch_size, L)\n",
    "            src_txt_mask: text mask (batch_size, N)\n",
    "        Returns:\n",
    "            output: highly text-irrelevant video features (batch_size, L, hidden_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpacking the input shapes, which will be used for broadcasting\n",
    "        batch_size, c_seq_len, dim = src_vid.shape\n",
    "        batch_size, q_seq_len, dim = src_txt.shape\n",
    "\n",
    "        # Random dropout (Regularization to enhance generalization)\n",
    "        context = self.dropout(src_vid)\n",
    "        query = self.dropout(src_txt)\n",
    "        \n",
    "        # Compute attention scores (Score = subres0 + subres1 + subres2)\n",
    "        # The subres0 is the contribution from video. Finally the W4C will be a video importance determiner\n",
    "        # subres1 from text. Finally the W4Q will be a text importance determiner\n",
    "        # subres2 from their interaction, this is the standard way to compute attention scores.\n",
    "        subres0 = torch.matmul(context, self.w4C).expand([-1, -1, q_seq_len])\n",
    "        subres1 = torch.matmul(query, self.w4Q).transpose(1, 2).expand([-1, c_seq_len, -1])\n",
    "        subres2 = torch.matmul(context * self.w4mlu, query.transpose(1, 2))\n",
    "\n",
    "        # Final attention score, where score[i,j] = i-th basic video feature + j-th basic text feature + their interaction\n",
    "        score = subres0 + subres1 + subres2\n",
    "\n",
    "        # The following code blocks compute our new video features based on cross-modal attention\n",
    "\n",
    "        # Step 1 Compute c2q and q2c attention\n",
    "        # Apply masks and compute attention\n",
    "        score_ = F.softmax(self.mask_logits(score, src_txt_mask.unsqueeze(1)), dim=2)\n",
    "        score_t = F.softmax(self.mask_logits(score, src_vid_mask.unsqueeze(2)), dim=1)\n",
    "        score_t = score_t.transpose(1, 2)\n",
    "        \n",
    "        # Context-to-query and query-to-context attention\n",
    "        c2q = torch.matmul(score_, src_txt)\n",
    "        q2c = torch.matmul(torch.matmul(score_, score_t), src_vid)\n",
    "        \n",
    "        # Step 2 combine original video features with attended features\n",
    "        # Concatenate features\n",
    "        feats = torch.cat([src_vid, c2q, torch.mul(src_vid, c2q), torch.mul(src_vid, q2c)], dim=2)\n",
    "        feats = feats.transpose(1, 2)\n",
    "        feats = self.cqa_linear(feats)\n",
    "        feats = feats.transpose(1, 2)\n",
    "\n",
    "        # After step 2, we have obtained text-aware video features (feats)\n",
    "\n",
    "        # Step 3, combine \"feats\" with \"pooled global text\" (pooled_src_txt) to get the FINAL text-aware video features\n",
    "        # Pooling query features\n",
    "        alpha = torch.tensordot(src_txt, self.weight, dims=1)\n",
    "        alpha = self.mask_logits(alpha, mask=src_txt_mask.unsqueeze(2))\n",
    "        alphas = F.softmax(alpha, dim=1)\n",
    "        pooled_src_txt = torch.matmul(src_txt.transpose(1, 2), alphas)\n",
    "        pooled_src_txt = pooled_src_txt.squeeze(2)\n",
    "        # The pooled_src_txt is the global text feature after pooling\n",
    "        \n",
    "        # Combine with pooled text\n",
    "        _, c_seq_len, _ = feats.shape\n",
    "        pooled_src_txt = pooled_src_txt.unsqueeze(1).repeat(1, c_seq_len, 1)\n",
    "        output = torch.cat([feats, pooled_src_txt], dim=2)\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.conv1d(output)\n",
    "        output = output.transpose(1, 2)\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value=-1e30):\n",
    "        mask = mask.type(torch.float32)\n",
    "        return inputs + (1.0 - mask) * mask_value\n",
    "\n",
    "# Transformer is used for global feature extraction, but we also need local feature extraction\n",
    "# Convolutional Block for local feature extraction\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"Residual convolutional blocks for extracting local features, This is the standard implementation of CNN blocks with residual connections.\"\"\"\n",
    "    def __init__(self, hidden_dim=256, n_blocks=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        class TheBlock(nn.Module):\n",
    "            def __init__(self, hidden_dim=256):\n",
    "                super().__init__()\n",
    "                self.conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "                self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "                self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "                self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "            def forward(self, x):\n",
    "                residual = x\n",
    "                out = F.relu(self.bn1(self.conv1(x)))\n",
    "                out = self.bn2(self.conv2(out))\n",
    "                return F.relu(out + residual)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([TheBlock(hidden_dim) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (seq_len, batch_size, hidden_dim)\n",
    "        Returns:\n",
    "            out: (seq_len, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        out = x.permute(1, 2, 0)  # (batch_size, hidden_dim, seq_len)\n",
    "        for layer in self.blocks:\n",
    "            out = layer(out)\n",
    "        out = out.permute(2, 0, 1)  # (seq_len, batch_size, hidden_dim)\n",
    "        return out\n",
    "\n",
    "print(\"V2T Extractor and Convolutional Block loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d38e3",
   "metadata": {},
   "source": [
    "#### Transformer Containers\n",
    "\n",
    "To keep the architecture modular we reimplement DETR-style encoder/decoder containers. They wrap individual layers, expose hooks for intermediate activations, and are reused across both moment retrieval and highlight detection heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder and Decoder containers\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Stack of Transformer encoder layers\"\"\"\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None, pos=None, **kwargs):\n",
    "        output = src\n",
    "        intermediate = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, \n",
    "                          pos=pos, **kwargs)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Stack of Transformer decoder layers with iterative refinement\"\"\"\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False,\n",
    "                 d_model=256, query_dim=2, keep_query_pos=False, \n",
    "                 query_scale_type=\"cond_elewise\", modulate_t_attn=False,\n",
    "                 bbox_embed_diff_each_layer=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "        self.query_dim = query_dim\n",
    "        \n",
    "        # Query scale module\n",
    "        self.query_scale_type = query_scale_type\n",
    "        if query_scale_type == \"cond_elewise\":\n",
    "            self.query_scale = MLP(d_model, d_model, d_model, 2)\n",
    "        elif query_scale_type == \"cond_scalar\":\n",
    "            self.query_scale = MLP(d_model, d_model, 1, 2)\n",
    "        elif query_scale_type == \"fix_elewise\":\n",
    "            self.query_scale = nn.Embedding(num_layers, d_model)\n",
    "        \n",
    "        self.ref_point_head = MLP(d_model, d_model, d_model, 2)\n",
    "        \n",
    "        # Bbox embedding for iterative refinement\n",
    "        if bbox_embed_diff_each_layer:\n",
    "            self.bbox_embed = nn.ModuleList([MLP(d_model, d_model, 2, 3) for _ in range(num_layers)])\n",
    "        else:\n",
    "            self.bbox_embed = MLP(d_model, d_model, 2, 3)\n",
    "        \n",
    "        # Initialize bbox_embed\n",
    "        if bbox_embed_diff_each_layer:\n",
    "            for bbox_embed in self.bbox_embed:\n",
    "                nn.init.constant_(bbox_embed.layers[-1].weight.data, 0)\n",
    "                nn.init.constant_(bbox_embed.layers[-1].bias.data, 0)\n",
    "        else:\n",
    "            nn.init.constant_(self.bbox_embed.layers[-1].weight.data, 0)\n",
    "            nn.init.constant_(self.bbox_embed.layers[-1].bias.data, 0)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.modulate_t_attn = modulate_t_attn\n",
    "        self.bbox_embed_diff_each_layer = bbox_embed_diff_each_layer\n",
    "        \n",
    "        if modulate_t_attn:\n",
    "            self.ref_anchor_head = MLP(d_model, d_model, 1, 2)\n",
    "        \n",
    "        if not keep_query_pos:\n",
    "            for layer_id in range(num_layers - 1):\n",
    "                self.layers[layer_id + 1].ca_qpos_proj = None\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None,\n",
    "                pos=None, refpoints_unsigmoid=None):\n",
    "\n",
    "        # output is our query notes\n",
    "        # first time it is all zero, then it is updated iteratively\n",
    "        output = tgt\n",
    "        # This will store the reference points at each layer\n",
    "        intermediate = []\n",
    "        # [mid, span], the shape is (batch_size, num_queries, 2)\n",
    "        reference_points = refpoints_unsigmoid.sigmoid()\n",
    "        ref_points = [reference_points]\n",
    "        \n",
    "        total_moe_loss = tgt.new_zeros(())\n",
    "        \n",
    "        # Layer by layer decoding\n",
    "        for layer_id, layer in enumerate(self.layers):\n",
    "            obj_center = reference_points[..., :self.query_dim]\n",
    "\n",
    "            # Generate sine embedding for query dynamically\n",
    "            query_sine_embed = gen_sineembed_for_position(obj_center)\n",
    "            query_pos = self.ref_point_head(query_sine_embed)\n",
    "            \n",
    "            # Apply transformation\n",
    "            if self.query_scale_type != \"fix_elewise\":\n",
    "                pos_transformation = 1 if layer_id == 0 else self.query_scale(output)\n",
    "            else:\n",
    "                pos_transformation = self.query_scale.weight[layer_id]\n",
    "            \n",
    "            query_sine_embed = query_sine_embed * pos_transformation\n",
    "            \n",
    "            # Modulated attention\n",
    "            if self.modulate_t_attn:\n",
    "                reft_cond = self.ref_anchor_head(output).sigmoid()\n",
    "                query_sine_embed *= (reft_cond[..., 0] / obj_center[..., 1]).unsqueeze(-1)\n",
    "\n",
    "            # Apply decoder layer (self attention and cross-attention)\n",
    "            output, layer_moe_loss = layer(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                                           pos=pos, query_pos=query_pos, query_sine_embed=query_sine_embed,\n",
    "                                           is_first=(layer_id == 0))\n",
    "            total_moe_loss = total_moe_loss + layer_moe_loss\n",
    "            \n",
    "            # Iterative box refinement\n",
    "            if self.bbox_embed is not None:\n",
    "                if self.bbox_embed_diff_each_layer:\n",
    "                    tmp = self.bbox_embed[layer_id](output)\n",
    "                else:\n",
    "                    tmp = self.bbox_embed(output)\n",
    "                tmp[..., :self.query_dim] += inverse_sigmoid(reference_points)\n",
    "                new_reference_points = tmp[..., :self.query_dim].sigmoid()\n",
    "                if layer_id != self.num_layers - 1:\n",
    "                    ref_points.append(new_reference_points)\n",
    "                reference_points = new_reference_points.detach()\n",
    "            \n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "        \n",
    "        total_moe_loss = total_moe_loss / max(self.num_layers, 1)\n",
    "        \n",
    "        if self.return_intermediate:\n",
    "            if self.bbox_embed is not None:\n",
    "                # torch.stack(intermediate), this is (num_layers, batch_size, num_queries, d_model)\n",
    "                # torch.stack(ref_points), this is (num_layers, batch_size, num_queries, 2)\n",
    "                return [torch.stack(intermediate).transpose(1, 2), \n",
    "                       torch.stack(ref_points).transpose(1, 2)], total_moe_loss\n",
    "            else:\n",
    "                return [torch.stack(intermediate).transpose(1, 2),\n",
    "                       reference_points.unsqueeze(0).transpose(1, 2)], total_moe_loss\n",
    "        \n",
    "        return output.unsqueeze(0), total_moe_loss\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    if activation == \"prelu\":\n",
    "        return nn.PReLU()\n",
    "    if activation == \"selu\":\n",
    "        return F.selu\n",
    "    raise RuntimeError(f\"activation should be relu/gelu/glu/prelu/selu, not {activation}.\")\n",
    "\n",
    "\n",
    "class T2V_TransformerEncoderLayer_no_global(nn.Module):\n",
    "    \"\"\"Text-to-Video cross-attention layer without global token\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    # A cross-attention layer from text to video and a feedforward layer\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None, pos=None, video_length=None, **kwargs):\n",
    "        assert video_length is not None\n",
    "        \n",
    "        pos_src = self.with_pos_embed(src, pos)\n",
    "\n",
    "        # note that from T2VExtractor, src is [video_length + text_length, batch_size, d_model]\n",
    "        # q pos_src is video features + video positional encodings\n",
    "        # k pos_src is text features + text positional encodings\n",
    "        # v src is text features\n",
    "        q, k, v = pos_src[:video_length], pos_src[video_length:], src[video_length:]\n",
    "        \n",
    "        # Create attention mask for video-to-text attention\n",
    "        qmask = src_key_padding_mask[:, :video_length].unsqueeze(2)\n",
    "        kmask = src_key_padding_mask[:, video_length:].unsqueeze(1)\n",
    "        attn_mask = torch.matmul(qmask.float(), kmask.float()).bool().repeat(self.nhead, 1, 1)\n",
    "        \n",
    "        src2 = self.self_attn(q, k, value=v, attn_mask=attn_mask,\n",
    "                             key_padding_mask=src_key_padding_mask[:, video_length:])[0]\n",
    "        \n",
    "        src2 = src[:video_length] + self.dropout1(src2)\n",
    "\n",
    "        src3 = self.norm1(src2)\n",
    "        src3 = self.linear2(self.dropout(self.activation(self.linear1(src3))))\n",
    "        \n",
    "        src2 = src2 + self.dropout2(src3)\n",
    "        src2 = self.norm2(src2)\n",
    "       \n",
    "        src = torch.cat([src2, src[video_length:]])\n",
    "        \n",
    "        return src\n",
    "\n",
    "print(\"Transformer Encoder and Decoder containers loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea7438",
   "metadata": {},
   "source": [
    "#### Decoder Stack & Heads\n",
    "\n",
    "With the container utilities in place, the next definitions wire up the Switch-Net decoder stack (formerly LD-DETR):\n",
    "- **LoopDecoder** performs iterative query refinement across multiple decoder layers.\n",
    "- **PredictionHeads** branch into the moment-retrieval classifier/regressor and the highlight-detection saliency head, keeping auxiliary outputs when deep supervision is enabled.\n",
    "\n",
    "These components consume the fused memory tensor produced earlier and emit the logits/spans/saliency signals that feed the training losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ceb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Fuser - combines multiple encoding stages\n",
    "class ConvolutionalFuser(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Fuser for efficient multimodal feature extraction\n",
    "    Pipeline: V2T Extractor -> T2V Encoder -> Encoder1 -> Conv Blocks -> Encoder2\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256, nhead=8, dim_feedforward=1024, dropout=0.1,\n",
    "                 activation=\"prelu\", num_v2t_encoder_layers=2, num_encoder1_layers=2,\n",
    "                 num_convolutional_blocks=5, num_encoder2_layers=2, normalize_before=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # V2T Extractor\n",
    "        self.v2t_extractor = V2TExtractor(hidden_dim, dropout)\n",
    "        \n",
    "        # T2V Encoder\n",
    "        self.t2v_encoder = TransformerEncoder(\n",
    "            T2V_TransformerEncoderLayer_no_global(hidden_dim, nhead, dim_feedforward, \n",
    "                                                  dropout, activation, normalize_before),\n",
    "            num_v2t_encoder_layers, None)\n",
    "        \n",
    "        # Transformer Encoder 1\n",
    "        self.transformer_encoder1 = TransformerEncoder(\n",
    "            TransformerEncoderLayer(hidden_dim, nhead, dim_feedforward, \n",
    "                                   dropout, activation, normalize_before),\n",
    "            num_encoder1_layers, None)\n",
    "        \n",
    "        # Convolutional Blocks\n",
    "        self.convolutional_block = ConvolutionalBlock(hidden_dim, num_convolutional_blocks)\n",
    "        \n",
    "        # Transformer Encoder 2\n",
    "        self.transformer_encoder2 = TransformerEncoder(\n",
    "            TransformerEncoderLayer(hidden_dim, nhead, dim_feedforward, \n",
    "                                   dropout, activation, normalize_before),\n",
    "            num_encoder2_layers, None)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        for module in [self.v2t_extractor, self.t2v_encoder, \n",
    "                      self.transformer_encoder1, self.transformer_encoder2]:\n",
    "            for n, p in module.named_parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src_vid, src_txt, src_vid_mask, src_txt_mask, pos_vid, pos_txt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vid: video features (batch_size, L, hidden_dim)\n",
    "            src_txt: text features (batch_size, N, hidden_dim)\n",
    "            src_vid_mask: video mask (batch_size, L)\n",
    "            src_txt_mask: text mask (batch_size, N)\n",
    "            pos_vid: video position encoding (batch_size, L, hidden_dim)\n",
    "            pos_txt: text position encoding (batch_size, N, hidden_dim)\n",
    "        Returns:\n",
    "            memory: fused multimodal features (L, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        # V2T Extractor, this return a video features with less text-irrelevant information\n",
    "        # Multi-Co Attention between video and text\n",
    "        src_vid = self.v2t_extractor(src_vid, src_txt, src_vid_mask, src_txt_mask)\n",
    "        \n",
    "        # T2V Encoder (cross-modal fusion)\n",
    "        src = torch.cat([src_vid, src_txt], dim=1)\n",
    "        # mask help the model ignore the padding positions\n",
    "        mask = torch.cat([src_vid_mask, src_txt_mask], dim=1).bool()\n",
    "        pos = torch.cat([pos_vid, pos_txt], dim=1)\n",
    "        video_length = src_vid.shape[1]\n",
    "        \n",
    "        # Transform to (seq_len, batch_size, hidden_dim) which will be used in Transformer\n",
    "        src = src.permute(1, 0, 2)  # (L+N, batch_size, hidden_dim)\n",
    "        pos = pos.permute(1, 0, 2)\n",
    "        \n",
    "        # Note we need to invert the mask for Transformer\n",
    "        src = self.t2v_encoder(src, src_key_padding_mask=~mask, pos=pos, \n",
    "                              video_length=video_length)\n",
    "        \n",
    "        # Since we only need video features for moment retrieval, we slice them here,and discard text features\n",
    "        src_vid = src[:video_length]\n",
    "        src_vid_mask = mask[:, :video_length]\n",
    "        pos_vid = pos[:video_length]\n",
    "        \n",
    "        # Transformer Encoder 1\n",
    "        memory = self.transformer_encoder1(src_vid, src_key_padding_mask=~src_vid_mask, \n",
    "                                          pos=pos_vid)\n",
    "        \n",
    "        # Convolutional Blocks (local feature extraction)\n",
    "        memory = self.convolutional_block(memory)\n",
    "        \n",
    "        # Transformer Encoder 2\n",
    "        memory = self.transformer_encoder2(memory, src_key_padding_mask=~src_vid_mask, \n",
    "                                          pos=pos_vid)\n",
    "        \n",
    "        return memory\n",
    "\n",
    "\n",
    "class LoopDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Loop Decoder for iterative refinement of predictions\n",
    "    Key innovation: Feed decoder output back as input multiple times\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=256, nhead=8, dim_feedforward=1024, dropout=0.1,\n",
    "                 activation=\"prelu\", normalize_before=False, keep_query_pos=False,\n",
    "                 num_decoder_layers=2, return_intermediate_dec=True, query_dim=2,\n",
    "                 query_scale_type=\"cond_elewise\", modulate_t_attn=True,\n",
    "                 bbox_embed_diff_each_layer=False, num_decoder_loops=3,\n",
    "                 moe_num_experts=8, moe_top_k=2, moe_load_balance_coef=0.01):\n",
    "        super().__init__()\n",
    "        self.num_decoder_loops = num_decoder_loops\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        self.transformer_decoder = TransformerDecoder(\n",
    "            TransformerDecoderLayer(hidden_dim, nhead, dim_feedforward, dropout,\n",
    "                                   activation, normalize_before, keep_query_pos=keep_query_pos,\n",
    "                                   moe_num_experts=moe_num_experts,\n",
    "                                   moe_top_k=moe_top_k,\n",
    "                                   moe_load_balance_coef=moe_load_balance_coef),\n",
    "            num_decoder_layers, nn.LayerNorm(hidden_dim),\n",
    "            return_intermediate=return_intermediate_dec, d_model=hidden_dim,\n",
    "            query_dim=query_dim, keep_query_pos=keep_query_pos,\n",
    "            query_scale_type=query_scale_type, modulate_t_attn=modulate_t_attn,\n",
    "            bbox_embed_diff_each_layer=bbox_embed_diff_each_layer)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        for n, p in self.transformer_decoder.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, tgt, memory, src_vid_mask, pos_vid, refpoint_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: zero matrix for queries (num_queries, batch_size, hidden_dim)\n",
    "            memory: multimodal features (L, batch_size, hidden_dim) < This is read-only\n",
    "            src_vid_mask: video mask (batch_size, L)\n",
    "            pos_vid: video position encoding (batch_size, L, hidden_dim)\n",
    "            refpoint_embed: reference point embeddings (num_queries, batch_size, 2)\n",
    "        Returns:\n",
    "            hs: decoded features (num_layers, batch_size, num_queries, hidden_dim)\n",
    "            reference: refined reference points (num_layers, batch_size, num_queries, 2)\n",
    "            moe_loss: scalar load-balancing loss aggregated across loops\n",
    "        \"\"\"\n",
    "        # Convert pos_vid to (L, batch_size, hidden_dim)\n",
    "        pos_vid = pos_vid.permute(1, 0, 2)\n",
    "        current_tgt = tgt\n",
    "        current_refpoints = refpoint_embed\n",
    "\n",
    "        # The results after multiple loops\n",
    "        hs = None  # hidden states\n",
    "        reference = None\n",
    "        total_moe_loss = memory.new_zeros(())\n",
    "        \n",
    "        # Iterative refinement through multiple decoder loops\n",
    "        for loop_idx in range(self.num_decoder_loops):\n",
    "            decoder_outputs, loop_moe_loss = self.transformer_decoder(\n",
    "                current_tgt,  # num queries\n",
    "                memory,  # the multimodal features, read-only\n",
    "                memory_key_padding_mask=~src_vid_mask,  # This tells the model which positions to ignore\n",
    "                pos=pos_vid,  # position encoding for memory\n",
    "                refpoints_unsigmoid=current_refpoints)  # reference points\n",
    "            hs, reference = decoder_outputs\n",
    "            total_moe_loss = total_moe_loss + loop_moe_loss\n",
    "            \n",
    "            if loop_idx < self.num_decoder_loops - 1:\n",
    "                current_tgt = hs[-1].transpose(0, 1)  # The notes from previous loop\n",
    "                current_refpoints = inverse_sigmoid(reference[-1]).transpose(0, 1)  # The refined reference points\n",
    "        \n",
    "        total_moe_loss = total_moe_loss / max(self.num_decoder_loops, 1)\n",
    "        \n",
    "        return hs, reference, total_moe_loss\n",
    "\n",
    "\n",
    "class VideoMomentRetrievalPredictionHead(nn.Module):\n",
    "    \"\"\"Prediction head for moment retrieval (classification + span regression)\"\"\"\n",
    "    def __init__(self, hidden_dim, span_loss_type):\n",
    "        super().__init__()\n",
    "        self.span_loss_type = span_loss_type\n",
    "        self.class_embed = nn.Linear(hidden_dim, 2)  # foreground/background\n",
    "        self.span_embed1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.span_embed2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.span_embed3 = nn.Linear(hidden_dim, 2)  # center, width\n",
    "\n",
    "    def forward(self, hs, reference):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hs: decoded features (num_layers, batch_size, num_queries, hidden_dim)\n",
    "            reference: reference points (num_layers, batch_size, num_queries, 2)\n",
    "        Returns:\n",
    "            pred_logits: classification logits\n",
    "            pred_spans: predicted spans\n",
    "            pred_logits_others: auxiliary predictions\n",
    "            pred_spans_others: auxiliary spans\n",
    "        \"\"\"\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        reference_before_sigmoid = inverse_sigmoid(reference)\n",
    "        # we deviate from original implementation for better stability\n",
    "        tmp = self.span_embed3(F.relu(self.span_embed2(F.relu(self.span_embed1(hs)))))\n",
    "        # Add the offset to the reference points (Residual connection)\n",
    "        outputs_coord = tmp + reference_before_sigmoid\n",
    "        \n",
    "        if self.span_loss_type == \"l1\":\n",
    "            outputs_coord = outputs_coord.sigmoid()  # normalize to [0, 1] (Sigmoid)\n",
    "        \n",
    "        pred_logits = outputs_class[-1]\n",
    "        pred_spans = outputs_coord[-1]\n",
    "        pred_logits_others = outputs_class[:-1]\n",
    "        pred_spans_others = outputs_coord[:-1]\n",
    "        \n",
    "        return pred_logits, pred_spans, pred_logits_others, pred_spans_others\n",
    "\n",
    "\n",
    "class HighlightDetectionPredictionHead(nn.Module):\n",
    "    \"\"\"Prediction head for highlight detection (saliency scores)\"\"\"\n",
    "    def __init__(self, hidden_dim=256, clip_len=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.clip_len = clip_len\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, num_layers=1, \n",
    "                         bidirectional=False, batch_first=True)\n",
    "        self.saliency_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, pred_logits, pred_spans, memory, src_vid):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_logits: classification logits (batch_size, num_queries, 2)\n",
    "            pred_spans: predicted spans (batch_size, num_queries, 2)\n",
    "            memory: multimodal features (batch_size, L, hidden_dim)\n",
    "            src_vid: original video features (batch_size, L, video_dim)\n",
    "        Returns:\n",
    "            saliency_scores: per-clip saliency scores (batch_size, L)\n",
    "        \"\"\"\n",
    "        video_length = memory.shape[1]\n",
    "        batch_size = memory.size(0)\n",
    "        \n",
    "        # Get top predicted moment\n",
    "        prob = F.softmax(pred_logits, -1)\n",
    "        scores = prob[..., 0]\n",
    "        sorted_indices = torch.sort(scores, dim=-1, descending=True)[1]\n",
    "        sorted_indices_max = sorted_indices[:, :1]\n",
    "        \n",
    "        # Convert spans to clip indices\n",
    "        spans = span_cxw_to_xx(pred_spans) * (video_length * self.clip_len)\n",
    "        spans = torch.floor(spans / self.clip_len)\n",
    "        \n",
    "        selected_values_max = spans[torch.arange(batch_size).unsqueeze(1), \n",
    "                                   sorted_indices_max].squeeze(1)\n",
    "        \n",
    "        # Extract features from predicted moments\n",
    "        sliced_samples = []\n",
    "        for i in range(batch_size):\n",
    "            start_time = int(selected_values_max[i, 0].clamp(0, video_length-1))\n",
    "            end_time = int(selected_values_max[i, 1].clamp(0, video_length-1))\n",
    "            sliced_sample = src_vid[i, start_time:end_time + 1, :]\n",
    "            \n",
    "            padding_size = video_length - sliced_sample.size(0)\n",
    "            if padding_size > 0:\n",
    "                padded_slice = F.pad(sliced_sample, (0, 0, 0, padding_size), value=0)\n",
    "            else:\n",
    "                padded_slice = sliced_sample[:video_length, :]\n",
    "            \n",
    "            sliced_samples.append(padded_slice)\n",
    "        \n",
    "        # Process with GRU and compute saliency\n",
    "        sliced_features = torch.stack(sliced_samples, dim=0)\n",
    "        _, hidden = self.gru(sliced_features)\n",
    "        hidden = hidden[-1, :, :]\n",
    "        \n",
    "        # Compute attention weights\n",
    "        weight = torch.matmul(hidden.unsqueeze(1), src_vid.transpose(1, 2)).squeeze(1)\n",
    "        # use weight to enhance memory\n",
    "        memory = memory * weight.unsqueeze(-1) + memory\n",
    "\n",
    "        # Project to saliency scores (use a mlp(saliency_proj) followed by sum and scaling)\n",
    "        saliency_scores = torch.sum(self.saliency_proj(memory), dim=-1) / np.sqrt(self.hidden_dim)\n",
    "        \n",
    "        return saliency_scores\n",
    "\n",
    "\n",
    "class PredictionHeads(nn.Module):\n",
    "    \"\"\"Combined prediction heads for moment retrieval and highlight detection\"\"\"\n",
    "    def __init__(self, hidden_dim, span_loss_type, clip_len=2, aux_loss=True):\n",
    "        super().__init__()\n",
    "        self.aux_loss = aux_loss\n",
    "        self.video_moment_retrieval_prediction_head = VideoMomentRetrievalPredictionHead(\n",
    "            hidden_dim, span_loss_type)\n",
    "        self.highlight_detection_prediction_head = HighlightDetectionPredictionHead(\n",
    "            hidden_dim, clip_len)\n",
    "\n",
    "    def forward(self, hs, reference, memory, src_vid):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hs: decoded features\n",
    "            reference: reference points\n",
    "            memory: multimodal features\n",
    "            src_vid: original video features\n",
    "        Returns:\n",
    "            pred_logits: classification predictions\n",
    "            pred_spans: span predictions\n",
    "            saliency_scores: highlight scores\n",
    "            aux_outputs: auxiliary outputs for deep supervision\n",
    "        \"\"\"\n",
    "        # Moment retrieval predictions\n",
    "        pred_logits, pred_spans, pred_logits_others, pred_spans_others = \\\n",
    "            self.video_moment_retrieval_prediction_head(hs, reference)\n",
    "        \n",
    "        # Highlight detection predictions\n",
    "        saliency_scores = self.highlight_detection_prediction_head(\n",
    "            pred_logits, pred_spans, memory, src_vid)\n",
    "        \n",
    "        # Auxiliary outputs\n",
    "        aux_outputs = None\n",
    "        if self.aux_loss:\n",
    "            aux_outputs = [{\"pred_logits\": a, \"pred_spans\": b} \n",
    "                          for a, b in zip(pred_logits_others, pred_spans_others)]\n",
    "        \n",
    "        return pred_logits, pred_spans, saliency_scores, aux_outputs\n",
    "\n",
    "print(\"Convolutional Fuser, Loop Decoder, and Prediction Heads loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e006765",
   "metadata": {},
   "source": [
    "### 4.5 Complete Switch-Net Model\n",
    "\n",
    "Now we combine all components into the complete Switch-Net model (renamed from LD-DETR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88856a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Switch-Net Model (formerly LD-DETR)\n",
    "class SwitchNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Switch-Net: Expert Decoder Refinement Network for video moment retrieval and highlight detection.\n",
    "\n",
    "    Backward compatibility: the legacy name LD-DETR remains available via an alias.\n",
    "    \"\"\"\n",
    "    def __init__(self, txt_dim, vid_dim, hidden_dim, num_queries, aux_loss=False,\n",
    "                 position_embedding=\"sine\", max_v_l=75, max_q_l=32, span_loss_type=\"l1\",\n",
    "                 use_txt_pos=False, aud_dim=0, queue_length=65536, momentum=0.995,\n",
    "                 distillation_coefficient=0.4, num_v2t_encoder_layers=2,\n",
    "                 num_encoder1_layers=2, num_convolutional_blocks=5,\n",
    "                 num_encoder2_layers=2, num_decoder_layers=2, num_decoder_loops=3,\n",
    "                 clip_len=2, moe_num_experts=8, moe_top_k=2, moe_load_balance_coef=0.01):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_queries = num_queries\n",
    "        self.span_loss_type = span_loss_type\n",
    "        self.max_v_l = max_v_l\n",
    "        self.max_q_l = max_q_l\n",
    "        self.use_txt_pos = use_txt_pos\n",
    "        self.momentum = momentum\n",
    "        self.clip_len = clip_len\n",
    "        self.moe_num_experts = moe_num_experts\n",
    "        self.moe_top_k = moe_top_k\n",
    "        self.moe_load_balance_coef = moe_load_balance_coef\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.position_embed, self.txt_position_embed = build_position_encoding(\n",
    "            hidden_dim, position_embedding, max_q_l)\n",
    "        \n",
    "        # Unimodal encoders\n",
    "        self.video_encoder = UnimodalEncoder(vid_dim + aud_dim, hidden_dim)\n",
    "        self.text_encoder = UnimodalEncoder(txt_dim, hidden_dim)\n",
    "        self.momentum_video_encoder = UnimodalEncoder(vid_dim + aud_dim, hidden_dim)\n",
    "        self.momentum_text_encoder = UnimodalEncoder(txt_dim, hidden_dim)\n",
    "        \n",
    "        self.model_pairs = [\n",
    "            [self.video_encoder, self.momentum_video_encoder],\n",
    "            [self.text_encoder, self.momentum_text_encoder],\n",
    "        ]\n",
    "        self._copy_params()\n",
    "        \n",
    "        # Distill align\n",
    "        self.distill_align = DistillAlign(hidden_dim, queue_length=queue_length,\n",
    "                                         alpha=distillation_coefficient)\n",
    "        \n",
    "        # Convolutional fuser\n",
    "        self.convolutional_fuser = ConvolutionalFuser(\n",
    "            hidden_dim=hidden_dim, num_v2t_encoder_layers=num_v2t_encoder_layers,\n",
    "            num_encoder1_layers=num_encoder1_layers,\n",
    "            num_convolutional_blocks=num_convolutional_blocks,\n",
    "            num_encoder2_layers=num_encoder2_layers)\n",
    "        \n",
    "        # Loop decoder\n",
    "        self.query_embed = nn.Embedding(num_queries, 2)\n",
    "        self.loop_decoder = LoopDecoder(\n",
    "            hidden_dim=hidden_dim, num_decoder_layers=num_decoder_layers,\n",
    "            num_decoder_loops=num_decoder_loops,\n",
    "            moe_num_experts=moe_num_experts,\n",
    "            moe_top_k=moe_top_k,\n",
    "            moe_load_balance_coef=moe_load_balance_coef)\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.prediction_heads = PredictionHeads(hidden_dim, span_loss_type,\n",
    "                                               clip_len, aux_loss)\n",
    "\n",
    "    def forward(self, src_txt, src_txt_mask, src_vid, src_vid_mask, vid=None,\n",
    "                qid=None, src_aud=None, src_aud_mask=None, epoch_i=0,\n",
    "                batch_idx=0, train_loader_length=0, targets=None, is_training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through Switch-Net.\n",
    "\n",
    "        Args:\n",
    "            src_vid: video features (batch_size, L, vid_dim)\n",
    "            src_txt: text features (batch_size, N, txt_dim)\n",
    "            src_vid_mask: video mask (batch_size, L)\n",
    "            src_txt_mask: text mask (batch_size, N)\n",
    "            src_aud: audio features (batch_size, L, aud_dim) [optional]\n",
    "            src_aud_mask: audio mask (batch_size, L) [optional]\n",
    "            epoch_i: current epoch\n",
    "            batch_idx: current batch index\n",
    "            train_loader_length: total batches per epoch\n",
    "            targets: ground truth targets\n",
    "            is_training: training mode flag\n",
    "            \n",
    "        Returns:\n",
    "            out: dictionary containing predictions and losses\n",
    "        \"\"\"\n",
    "        # Concatenate audio if provided\n",
    "        if src_aud is not None:\n",
    "            src_vid = torch.cat([src_vid, src_aud], dim=2)\n",
    "        \n",
    "        # Unimodal encoders\n",
    "        src_vid_copy = src_vid\n",
    "        src_txt_copy = src_txt\n",
    "        src_vid = self.video_encoder(src_vid)\n",
    "        src_txt = self.text_encoder(src_txt)\n",
    "\n",
    "        # We don't care about gradients for momentum encoders\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update()\n",
    "            src_vid_m = self.momentum_video_encoder(src_vid_copy)\n",
    "            src_txt_m = self.momentum_text_encoder(src_txt_copy)\n",
    "        \n",
    "        # Distill align (We only use the DistillAlign module to compute loss_align and loss_sim during training)\n",
    "        loss_align, loss_sim = self.distill_align(\n",
    "            F.normalize(src_vid.mean(1), dim=-1),\n",
    "            F.normalize(src_txt.mean(1), dim=-1),\n",
    "            F.normalize(src_vid_m.mean(1), dim=-1).detach(),\n",
    "            F.normalize(src_txt_m.mean(1), dim=-1).detach(),\n",
    "            epoch_i=epoch_i, batch_idx=batch_idx,\n",
    "            train_loader_length=train_loader_length, is_training=is_training)\n",
    "        \n",
    "        # Positional embedding\n",
    "        pos_vid = self.position_embed(src_vid, src_vid_mask)\n",
    "        pos_txt = (self.txt_position_embed(src_txt) if self.use_txt_pos \n",
    "                  else torch.zeros_like(src_txt))\n",
    "        \n",
    "        # Convolutional fuser\n",
    "        memory = self.convolutional_fuser(src_vid, src_txt, src_vid_mask.bool(),\n",
    "                                         src_txt_mask.bool(), pos_vid, pos_txt)\n",
    "        \n",
    "        # Loop decoder\n",
    "        _, bs, d = memory.shape  # (L, batch_size, hidden_dim)\n",
    "\n",
    "        # initialize reference point embeddings\n",
    "        refpoint_embed = self.query_embed.weight.unsqueeze(1).repeat(1, bs, 1)\n",
    "\n",
    "        # initialize tgt as zeros matrix, where tgt is the query notes\n",
    "        tgt = torch.zeros(refpoint_embed.shape[0], bs, d).to(src_vid.device)\n",
    "        \n",
    "        hs, reference, moe_loss = self.loop_decoder(tgt, memory, src_vid_mask.bool(),\n",
    "                                                    pos_vid, refpoint_embed)\n",
    "        memory = memory.transpose(0, 1)  # (batch_size, L, hidden_dim)\n",
    "        \n",
    "        # Prediction heads\n",
    "        pred_logits, pred_spans, saliency_scores, aux_outputs = \\\n",
    "            self.prediction_heads(hs, reference, memory, src_vid)\n",
    "        \n",
    "        # Prepare output\n",
    "        out = {\n",
    "            \"loss_align\": loss_align,\n",
    "            \"loss_sim\": loss_sim,\n",
    "            \"loss_moe_balance\": moe_loss,\n",
    "            \"video_mask\": src_vid_mask,\n",
    "            \"pred_logits\": pred_logits,\n",
    "            \"pred_spans\": pred_spans,\n",
    "            \"saliency_scores\": saliency_scores,\n",
    "            \"aux_outputs\": aux_outputs if aux_outputs is not None else []\n",
    "        }\n",
    "        \n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _copy_params(self):\n",
    "        \"\"\"Copy parameters to momentum encoders\"\"\"\n",
    "        for model_pair in self.model_pairs:\n",
    "            for param, param_m in zip(model_pair[0].parameters(), \n",
    "                                     model_pair[1].parameters()):\n",
    "                param_m.data.copy_(param.data)\n",
    "                param_m.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update(self):\n",
    "        \"\"\"Update momentum encoders\"\"\"\n",
    "        for model_pair in self.model_pairs:\n",
    "            for param, param_m in zip(model_pair[0].parameters(),\n",
    "                                     model_pair[1].parameters()):\n",
    "                param_m.data = param_m.data * self.momentum + \\\n",
    "                              param.data * (1.0 - self.momentum)\n",
    "\n",
    "# Legacy alias to keep external demos functional\n",
    "LD_DETR = SwitchNet\n",
    "print(\"Complete Switch-Net model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ef8b7",
   "metadata": {},
   "source": [
    "### 4.6 Hungarian Matcher and Loss Criterion\n",
    "\n",
    "The Hungarian algorithm matches predictions to ground truth, and the loss criterion computes all training losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af945838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hungarian Matcher\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"\n",
    "    Hungarian Matcher for bipartite matching between predictions and ground truth\n",
    "    Uses the Hungarian algorithm to find optimal assignment\n",
    "    \"\"\"\n",
    "    def __init__(self, cost_class=1, cost_span=1, cost_giou=1, \n",
    "                 span_loss_type=\"l1\", max_v_l=75):\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_span = cost_span\n",
    "        self.cost_giou = cost_giou\n",
    "        self.span_loss_type = span_loss_type\n",
    "        self.max_v_l = max_v_l\n",
    "        self.foreground_label = 0\n",
    "        assert cost_class != 0 or cost_span != 0 or cost_giou != 0, \\\n",
    "            \"All costs cannot be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Performs bipartite matching\n",
    "        \n",
    "        Args:\n",
    "            outputs: dict with \"pred_spans\" and \"pred_logits\"\n",
    "            targets: dict with ground truth annotations\n",
    "        \n",
    "        Returns:\n",
    "            List of (pred_idx, target_idx) tuples for each batch element\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_spans\"].shape[:2]\n",
    "        targets = targets[\"span_labels\"]\n",
    "        \n",
    "        # Flatten predictions\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)\n",
    "        tgt_spans = torch.cat([v[\"spans\"] for v in targets])\n",
    "        tgt_ids = torch.full([len(tgt_spans)], self.foreground_label,\n",
    "                            dtype=torch.long, device=out_prob.device)\n",
    "        \n",
    "        # Classification cost\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "        \n",
    "        if self.span_loss_type == \"l1\":\n",
    "            out_spans = outputs[\"pred_spans\"].flatten(0, 1)\n",
    "            \n",
    "            # L1 cost\n",
    "            cost_span = torch.cdist(out_spans, tgt_spans, p=1)\n",
    "            \n",
    "            # GIoU cost\n",
    "            cost_giou = -generalized_temporal_iou(\n",
    "                span_cxw_to_xx(out_spans), span_cxw_to_xx(tgt_spans))\n",
    "        else:\n",
    "            # Cross-entropy cost for classification-based span prediction\n",
    "            pred_spans = outputs[\"pred_spans\"]\n",
    "            pred_spans = pred_spans.view(bs * num_queries, 2, self.max_v_l).softmax(-1)\n",
    "            cost_span = -pred_spans[:, 0][:, tgt_spans[:, 0]] - \\\n",
    "                       pred_spans[:, 1][:, tgt_spans[:, 1]]\n",
    "            cost_giou = 0\n",
    "        \n",
    "        # Final cost matrix\n",
    "        C = self.cost_span * cost_span + self.cost_giou * cost_giou + \\\n",
    "            self.cost_class * cost_class\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "        \n",
    "        sizes = [len(v[\"spans\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) \n",
    "                  for i, c in enumerate(C.split(sizes, -1))]\n",
    "        \n",
    "        return [(torch.as_tensor(i, dtype=torch.int64),\n",
    "                torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "\n",
    "def build_matcher(cost_span=10, cost_giou=1, cost_class=4, \n",
    "                  span_loss_type=\"l1\", max_v_l=75):\n",
    "    \"\"\"Build Hungarian Matcher\"\"\"\n",
    "    return HungarianMatcher(cost_class=cost_class, cost_span=cost_span,\n",
    "                           cost_giou=cost_giou, span_loss_type=span_loss_type,\n",
    "                           max_v_l=max_v_l)\n",
    "\n",
    "print(\"Hungarian Matcher loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion(nn.Module):\n",
    "    \"\"\"Loss computation for Switch-Net (compatible with the legacy LD-DETR name).\"\"\"\n",
    "    def __init__(self, matcher, weight_dict, losses, span_loss_type=\"l1\", max_v_l=75,\n",
    "                 eos_coef=0.1, saliency_margin=0.2, saliency_label_scale=12, use_matcher=True):\n",
    "        super().__init__()\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.span_loss_type = span_loss_type\n",
    "        self.max_v_l = max_v_l\n",
    "        self.saliency_margin = saliency_margin\n",
    "        self.saliency_label_scale = max(int(saliency_label_scale), 1)\n",
    "        self.use_matcher = use_matcher\n",
    "        \n",
    "        self.foreground_label = 0\n",
    "        self.background_label = 1\n",
    "        self.eos_coef = eos_coef\n",
    "        \n",
    "        empty_weight = torch.ones(2)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer(\"empty_weight\", empty_weight)\n",
    "        \n",
    "    def loss_spans(self, outputs, targets, indices):\n",
    "        assert \"pred_spans\" in outputs\n",
    "        targets = targets[\"span_labels\"]\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_spans = outputs[\"pred_spans\"][idx]\n",
    "        tgt_spans = torch.cat([t[\"spans\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "        \n",
    "        if self.span_loss_type == \"l1\":\n",
    "            loss_span = F.l1_loss(src_spans, tgt_spans, reduction=\"none\")\n",
    "            loss_giou = 1 - torch.diag(generalized_temporal_iou(\n",
    "                span_cxw_to_xx(src_spans), span_cxw_to_xx(tgt_spans)))\n",
    "        else:\n",
    "            n_spans = src_spans.shape[0]\n",
    "            src_spans = src_spans.view(n_spans, 2, self.max_v_l).transpose(1, 2)\n",
    "            loss_span = F.cross_entropy(src_spans, tgt_spans, reduction=\"none\")\n",
    "            loss_giou = loss_span.new_zeros([1])\n",
    "        \n",
    "        return {\"loss_span\": loss_span.mean(), \"loss_giou\": loss_giou.mean()}\n",
    "        \n",
    "    def loss_labels(self, outputs, targets, indices, log=True):\n",
    "        assert \"pred_logits\" in outputs\n",
    "        src_logits = outputs[\"pred_logits\"]\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        \n",
    "        target_classes = torch.full(src_logits.shape[:2], self.background_label,\n",
    "                                   dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = self.foreground_label\n",
    "        \n",
    "        src_logits_flat = src_logits.flatten(0, 1)\n",
    "        target_classes_flat = target_classes.flatten()\n",
    "        \n",
    "        loss_ce = F.cross_entropy(\n",
    "            src_logits_flat.to(torch.float32),\n",
    "            target_classes_flat,\n",
    "            weight=self.empty_weight.to(src_logits.device)\n",
    "        )\n",
    "        \n",
    "        losses = {\"loss_label\": loss_ce}\n",
    "        \n",
    "        if log and target_classes_flat.numel() > 0:\n",
    "            with torch.no_grad():\n",
    "                class_err = 100 - accuracy(src_logits_flat, target_classes_flat)[0]\n",
    "            losses[\"class_error\"] = class_err\n",
    "        elif log:\n",
    "            losses[\"class_error\"] = torch.tensor(100.0, device=src_logits.device)\n",
    "        \n",
    "        return losses\n",
    "        \n",
    "    def loss_saliency(self, outputs, targets, indices, log=True):\n",
    "        if \"saliency_pos_labels\" not in targets:\n",
    "            zero = torch.tensor(0.0, device=outputs[\"saliency_scores\"].device)\n",
    "            return {\"loss_saliency\": zero}\n",
    "        \n",
    "        vid_token_mask = outputs[\"video_mask\"]\n",
    "        saliency_scores = outputs[\"saliency_scores\"].clone()\n",
    "        saliency_contrast_label = targets[\"saliency_all_labels\"]\n",
    "        saliency_scores = (vid_token_mask * saliency_scores + (1.0 - vid_token_mask) * -1e3)\n",
    "        \n",
    "        tau = 0.5\n",
    "        loss_rank_contrastive = 0.0\n",
    "        max_rank = max(self.saliency_label_scale, 1)\n",
    "        for rand_idx in range(1, max_rank + 1):\n",
    "            drop_mask = ~(saliency_contrast_label > 100)\n",
    "            pos_mask = saliency_contrast_label >= rand_idx\n",
    "            if torch.sum(pos_mask) == 0:\n",
    "                continue\n",
    "            batch_drop_mask = torch.sum(pos_mask, dim=1) > 0\n",
    "            cur_saliency_scores = saliency_scores * drop_mask / tau + ~drop_mask * -1e3\n",
    "            logits = cur_saliency_scores - torch.max(cur_saliency_scores, dim=1, keepdim=True)[0]\n",
    "            exp_logits = torch.exp(logits)\n",
    "            log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-6)\n",
    "            mean_log_prob_pos = (pos_mask * log_prob * vid_token_mask).sum(1) / (pos_mask.sum(1) + 1e-6)\n",
    "            loss = -mean_log_prob_pos * batch_drop_mask\n",
    "            loss_rank_contrastive = loss_rank_contrastive + loss.mean()\n",
    "        if max_rank > 0:\n",
    "            loss_rank_contrastive = loss_rank_contrastive / max_rank\n",
    "        \n",
    "        saliency_scores = outputs[\"saliency_scores\"]\n",
    "        pos_indices = targets[\"saliency_pos_labels\"]\n",
    "        neg_indices = targets[\"saliency_neg_labels\"]\n",
    "        num_pairs = pos_indices.shape[1]\n",
    "        batch_indices = torch.arange(len(saliency_scores)).to(saliency_scores.device)\n",
    "        pos_scores = torch.stack([saliency_scores[batch_indices, pos_indices[:, col_idx]]\n",
    "                                 for col_idx in range(num_pairs)], dim=1)\n",
    "        neg_scores = torch.stack([saliency_scores[batch_indices, neg_indices[:, col_idx]]\n",
    "                                 for col_idx in range(num_pairs)], dim=1)\n",
    "        loss_saliency = (torch.clamp(self.saliency_margin + neg_scores - pos_scores,\n",
    "                                     min=0).sum() / (len(pos_scores) * num_pairs) * 2)\n",
    "        loss_saliency = loss_saliency + loss_rank_contrastive\n",
    "        \n",
    "        return {\"loss_saliency\": loss_saliency}\n",
    "        \n",
    "    def loss_align(self, outputs, targets, indices, log=True):\n",
    "        return {\"loss_align\": outputs[\"loss_align\"]}\n",
    "        \n",
    "    def loss_sim(self, outputs, targets, indices, log=True):\n",
    "        return {\"loss_sim\": outputs[\"loss_sim\"]}\n",
    "        \n",
    "    def loss_moe_balance(self, outputs, targets, indices, log=True):\n",
    "        moe_loss = outputs.get(\"loss_moe_balance\")\n",
    "        base_tensor = self.empty_weight\n",
    "        if moe_loss is None:\n",
    "            moe_loss = base_tensor.new_tensor(0.0)\n",
    "        elif not isinstance(moe_loss, torch.Tensor):\n",
    "            moe_loss = base_tensor.new_tensor(moe_loss)\n",
    "        else:\n",
    "            moe_loss = moe_loss.to(base_tensor.device)\n",
    "        return {\"loss_moe_balance\": moe_loss}\n",
    "        \n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        batch_idx = torch.cat([torch.full_like(src, i)\n",
    "                               for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        if self.use_matcher:\n",
    "            indices = self.matcher(outputs, targets)\n",
    "        else:\n",
    "            indices = [(torch.arange(outputs[\"pred_spans\"].shape[1], device=outputs[\"pred_spans\"].device),\n",
    "                        torch.arange(t[\"spans\"].shape[0], device=outputs[\"pred_spans\"].device))\n",
    "                       for t in targets[\"span_labels\"]]\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(getattr(self, f\"loss_{loss}\")(outputs, targets, indices))\n",
    "        \n",
    "        if \"aux_outputs\" in outputs and outputs[\"aux_outputs\"]:\n",
    "            for aux_idx, aux_outputs in enumerate(outputs[\"aux_outputs\"]):\n",
    "                if self.use_matcher:\n",
    "                    aux_indices = self.matcher(aux_outputs, targets)\n",
    "                else:\n",
    "                    aux_indices = [(torch.arange(aux_outputs[\"pred_spans\"].shape[1], device=aux_outputs[\"pred_spans\"].device),\n",
    "                                    torch.arange(t[\"spans\"].shape[0], device=aux_outputs[\"pred_spans\"].device))\n",
    "                                   for t in targets[\"span_labels\"]]\n",
    "                for loss in self.losses:\n",
    "                    if loss in {\"saliency\", \"align\", \"sim\", \"moe_balance\"}:\n",
    "                        continue\n",
    "                    l_dict = getattr(self, f\"loss_{loss}\")(aux_outputs, targets, aux_indices)\n",
    "                    l_dict = {k + f\"_{aux_idx}\": v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331da127",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "**1. Data Loading**\n",
    "- Current notebook has placeholder data paths\n",
    "- You need to implement actual dataset loading\n",
    "- Create proper dataloaders with collate functions\n",
    "- Ensure features are pre-extracted (CLIP, SlowFast)\n",
    "\n",
    "**2. Queue Length**\n",
    "- `queue_length` in Distill Align must be divisible by batch size\n",
    "- Default: 65536 (works with batch sizes like 32, 64, 128)\n",
    "- If you get errors, adjust queue_length or batch_size\n",
    "\n",
    "**3. Feature Extraction**\n",
    "- Video features: SlowFast (2304-dim) + CLIP (512-dim) = 2816-dim\n",
    "- Text features: CLIP text encoder (512-dim)\n",
    "- Ensure your features match these dimensions\n",
    "\n",
    "**4. Training Data Format**\n",
    "The model expects data in this format:\n",
    "```python\n",
    "{\n",
    "    \"qid\": query_id,\n",
    "    \"query\": \"text description\",\n",
    "    \"duration\": video_duration,\n",
    "    \"vid\": video_id,\n",
    "    \"relevant_windows\": [[start, end]]  # in seconds\n",
    "}\n",
    "```\n",
    "\n",
    "### Testing the Notebook:\n",
    "\n",
    "Before full training, test with:\n",
    "1. Small batch size (e.g., 2-4 samples)\n",
    "2. Few epochs (e.g., 2-3)\n",
    "3. Reduced max_v_l (e.g., 30 instead of 75)\n",
    "4. Check all losses are computed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bfe1b",
   "metadata": {},
   "source": [
    "## 5. Configuration and Hyperparameters\n",
    "\n",
    "This section instantiates a single `Config` object that centralises every tunable setting: model depth, optimiser defaults, and the custom loss weights we just re-balanced. The comments in the cell call out the rationale for each group of parameters so future adjustments can be made without hunting through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration container for training hyperparameters.\"\"\"\n",
    "    def __init__(self):\n",
    "        # General settings\n",
    "        self.device = device\n",
    "        self.hidden_dim = 256\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.txt_dim = 512\n",
    "        self.vid_dim = 514\n",
    "        self.aud_dim = 0\n",
    "        \n",
    "        # Query/video parameters\n",
    "        self.num_queries = 10\n",
    "        self.max_q_l = 32\n",
    "        self.max_v_l = -1  # use full sequence length by default\n",
    "        self.clip_len = 1\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position_embedding = \"sine\"\n",
    "        self.use_txt_pos = False\n",
    "        \n",
    "        # Distill Align parameters\n",
    "        self.queue_length = 65536\n",
    "        self.momentum = 0.995\n",
    "        self.distillation_coefficient = 0.3\n",
    "        \n",
    "        # Encoder parameters\n",
    "        self.num_v2t_encoder_layers = 2\n",
    "        self.num_encoder1_layers = 2\n",
    "        self.num_convolutional_blocks = 4\n",
    "        self.num_encoder2_layers = 2\n",
    "        \n",
    "        # Decoder parameters\n",
    "        self.num_decoder_layers = 2\n",
    "        self.num_decoder_loops = 3\n",
    "        self.moe_num_experts = 8\n",
    "        self.moe_top_k = 2\n",
    "        self.moe_load_balance_coef = 1.0\n",
    "        \n",
    "        # Loss options\n",
    "        self.span_loss_type = \"l1\"\n",
    "        self.aux_loss = True\n",
    "        \n",
    "        # Optimization parameters\n",
    "        self.lr = 1e-4\n",
    "        self.weight_decay = 1e-4\n",
    "        self.n_epoch = 200\n",
    "        self.batch_size = 64\n",
    "        self.grad_clip = 0.1\n",
    "        \n",
    "        # Loss weights \n",
    "        self.span_loss_coef = 10\n",
    "        self.giou_loss_coef = 1\n",
    "        self.label_loss_coef = 4\n",
    "        self.saliency_loss_coef = 1.5\n",
    "        self.align_loss_coef = 0.6\n",
    "        self.sim_loss_coef = 0.4\n",
    "        self.moe_loss_coef = 0.01\n",
    "        \n",
    "        # Saliency and span stabilization\n",
    "        self.saliency_margin = 0.2\n",
    "        self.saliency_label_scale = 3.0\n",
    "        self.saliency_warmup_epochs = 3\n",
    "        self.align_warmup_epochs = 2\n",
    "        self.min_span_width_ratio = 0.0\n",
    "\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "max_v_desc = \"full sequence\" if config.max_v_l <= 0 else f\"{config.max_v_l} clips\"\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"- Hidden dimension: {config.hidden_dim}\")\n",
    "print(f\"- Number of queries: {config.num_queries}\")\n",
    "print(f\"- Max video length: {max_v_desc}\")\n",
    "print(f\"- Clip length (seconds per feature): {config.clip_len}\")\n",
    "print(f\"- Batch size: {config.batch_size}\")\n",
    "print(f\"- Learning rate: {config.lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6b481",
   "metadata": {},
   "source": [
    "## 6. Data Loading\n",
    "\n",
    "Before instantiating the datasets we pin down the absolute paths to the JSON annotations and pre-computed CLIP features. Keeping this configuration in one place makes it obvious how to repoint the notebook to a new dataset split or feature directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d005b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths - adjust these to your actual data paths\n",
    "DATA_ROOT = \"data\"\n",
    "TRAIN_PATH = f\"{DATA_ROOT}/train.jsonl\"\n",
    "TEST_PATH = f\"{DATA_ROOT}/test.jsonl\"\n",
    "\n",
    "# Feature directories\n",
    "FEATURE_ROOT = f\"{DATA_ROOT}/features\"\n",
    "\n",
    "# The Video Feature directories (can include multiple types)\n",
    "VIDEO_FEAT_DIRS = [\n",
    "    f\"{FEATURE_ROOT}/video_clip_features\",\n",
    "    # f\"{FEATURE_ROOT}/video_slowfast_features\",\n",
    " ]  # TEF channels are appended automatically during loading\n",
    "\n",
    "QUERY_FEAT_DIR = f\"{FEATURE_ROOT}/query_clip_features\"\n",
    "\n",
    "print(\"Data paths configured:\")\n",
    "print(f\"- Train data: {TRAIN_PATH}\")\n",
    "print(f\"- Test data: {TEST_PATH}\")\n",
    "print(\"- Video features:\")\n",
    "for path in VIDEO_FEAT_DIRS:\n",
    "    print(f\"    {path}\")\n",
    "print(f\"- Query features: {QUERY_FEAT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ab1d7",
   "metadata": {},
   "source": [
    "## 7. Model Initialization\n",
    "\n",
    "Now let's initialize the Switch-Net model (formerly LD-DETR) with our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Switch-Net model\n",
    "model = SwitchNet(\n",
    "    txt_dim=config.txt_dim,\n",
    "    vid_dim=config.vid_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    "    num_queries=config.num_queries,\n",
    "    aux_loss=config.aux_loss,\n",
    "    position_embedding=config.position_embedding,\n",
    "    max_v_l=config.max_v_l,\n",
    "    max_q_l=config.max_q_l,\n",
    "    span_loss_type=config.span_loss_type,\n",
    "    use_txt_pos=config.use_txt_pos,\n",
    "    aud_dim=config.aud_dim,\n",
    "    queue_length=config.queue_length,\n",
    "    momentum=config.momentum,\n",
    "    distillation_coefficient=config.distillation_coefficient,\n",
    "    num_v2t_encoder_layers=config.num_v2t_encoder_layers,\n",
    "    num_encoder1_layers=config.num_encoder1_layers,\n",
    "    num_convolutional_blocks=config.num_convolutional_blocks,\n",
    "    num_encoder2_layers=config.num_encoder2_layers,\n",
    "    num_decoder_layers=config.num_decoder_layers,\n",
    "    num_decoder_loops=config.num_decoder_loops,\n",
    "    clip_len=config.clip_len,\n",
    "    moe_num_experts=config.moe_num_experts,\n",
    "    moe_top_k=config.moe_top_k,\n",
    "    moe_load_balance_coef=config.moe_load_balance_coef,\n",
    " ).to(config.device)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5293ec",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Matcher\n",
    "\n",
    "We mirror Switch-Net's (formerly LD-DETR) matching strategy (Hungarian assignment on span + GIoU + class costs) and then assemble a weighted loss dictionary. The coefficients reflect our latest diagnostics: span and classification remain dominant, while saliency, alignment, and similarity receive tempered weights so they still guide learning without overwhelming the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build matcher for Hungarian algorithm\n",
    "matcher = build_matcher(\n",
    "    cost_span=config.span_loss_coef,\n",
    "    cost_giou=config.giou_loss_coef,\n",
    "    cost_class=config.label_loss_coef,\n",
    "    span_loss_type=config.span_loss_type,\n",
    "    max_v_l=config.max_v_l\n",
    " )\n",
    "\n",
    "# Weight dictionary for losses\n",
    "weight_dict = {\n",
    "    \"loss_span\": config.span_loss_coef,\n",
    "    \"loss_giou\": config.giou_loss_coef,\n",
    "    \"loss_label\": config.label_loss_coef,\n",
    "    \"loss_saliency\": config.saliency_loss_coef,\n",
    "    \"loss_align\": config.align_loss_coef,           # DistillAlign loss weight\n",
    "    \"loss_sim\": config.sim_loss_coef,              # DistillAlign similarity loss weight\n",
    "    \"loss_moe_balance\": config.moe_loss_coef,      # MoE load balancing loss weight\n",
    " }\n",
    "\n",
    "# Add auxiliary losses\n",
    "if config.aux_loss:\n",
    "    aux_weight_dict = {}\n",
    "    for i in range(max(config.num_decoder_layers - 1, 0)):\n",
    "        aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()\n",
    "                               if k not in [\"loss_saliency\", \"loss_align\", \"loss_sim\", \"loss_moe_balance\"]})\n",
    "    weight_dict.update(aux_weight_dict)\n",
    "\n",
    "# Initialize criterion\n",
    "criterion = SetCriterion(\n",
    "    matcher=matcher,\n",
    "    weight_dict=weight_dict,\n",
    "    losses=[\"spans\", \"labels\", \"saliency\", \"align\", \"sim\", \"moe_balance\"],\n",
    "    span_loss_type=config.span_loss_type,\n",
    "    max_v_l=config.max_v_l,\n",
    "    saliency_margin=config.saliency_margin,\n",
    "    saliency_label_scale=config.saliency_label_scale,\n",
    "    use_matcher=True\n",
    " ).to(config.device)\n",
    "\n",
    "print(\"Loss function and matcher initialized\")\n",
    "print(f\"Loss weights: {list(weight_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0774d",
   "metadata": {},
   "source": [
    "## 9. Optimizer and Scheduler\n",
    "\n",
    "We use AdamW with weight decay and a cosine annealing schedule that gradually decays the learning rate over the full training horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=max(config.n_epoch, 1),\n",
    "    eta_min=0.0\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={config.lr}, weight_decay={config.weight_decay})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR (T_max={max(config.n_epoch, 1)}, eta_min=0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5a4c6",
   "metadata": {},
   "source": [
    "## 10. Training Loop\n",
    "\n",
    "The following cell defines the main training loop helper functions for Switch-Net, including:\n",
    "\n",
    "- `prepare_batch_inputs`: Prepares and moves batch data to the correct device.\n",
    "- `train_one_epoch`: Runs a full training epoch with detailed diagnostics (loss breakdowns, gradient norm tracking, etc.).\n",
    "- Utility functions for logging and loss formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a129e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_LOSS_KEYS = [\n",
    "    \"loss_span\",\n",
    "    \"loss_giou\",\n",
    "    \"loss_label\",\n",
    "    \"loss_saliency\",\n",
    "    \"loss_align\",\n",
    "    \"loss_sim\",\n",
    "    \"loss_moe_balance\",\n",
    "]\n",
    "\n",
    "def prepare_batch_inputs(batch_data, device):\n",
    "    \"\"\"\n",
    "    Prepare batch inputs for model.\n",
    "    \"\"\"\n",
    "    model_inputs, targets = batch_data\n",
    "    \n",
    "    model_inputs = {\n",
    "        'src_vid': model_inputs['src_vid'].to(device),\n",
    "        'src_txt': model_inputs['src_txt'].to(device),\n",
    "        'src_vid_mask': model_inputs['src_vid_mask'].to(device),\n",
    "        'src_txt_mask': model_inputs['src_txt_mask'].to(device),\n",
    "    }\n",
    "    \n",
    "    processed_targets = {\n",
    "        'span_labels': [\n",
    "            {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "             for k, v in t.items()}\n",
    "            for t in targets['span_labels']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if 'saliency_pos_labels' in targets:\n",
    "        processed_targets['saliency_pos_labels'] = targets['saliency_pos_labels'].to(device)\n",
    "        processed_targets['saliency_neg_labels'] = targets['saliency_neg_labels'].to(device)\n",
    "    if 'saliency_all_labels' in targets:\n",
    "        processed_targets['saliency_all_labels'] = targets['saliency_all_labels'].to(device)\n",
    "        processed_targets['relevant_clips'] = processed_targets['saliency_all_labels']\n",
    "    \n",
    "    return model_inputs, processed_targets\n",
    "\n",
    "\n",
    "def _canonical_loss_key(name: str) -> Optional[str]:\n",
    "    for key in PRIMARY_LOSS_KEYS:\n",
    "        if name.startswith(key):\n",
    "            return key\n",
    "    if name == \"class_error\":\n",
    "        return \"class_error\"\n",
    "    return None\n",
    "\n",
    "def _format_loss_dict(loss_dict: Dict[str, torch.Tensor]) -> str:\n",
    "    segments = []\n",
    "    for key in sorted(loss_dict.keys()):\n",
    "        if key.startswith(\"loss_\") or key == \"class_error\":\n",
    "            value = loss_dict[key]\n",
    "            value = value.item() if isinstance(value, torch.Tensor) else float(value)\n",
    "            segments.append(f\"{key}:{value:.3f}\")\n",
    "    return \" | \".join(segments)\n",
    "\n",
    "def _compute_grad_norm(parameters) -> float:\n",
    "    norms = [p.grad.detach().data.norm(2) for p in parameters if p.grad is not None]\n",
    "    if not norms:\n",
    "        return 0.0\n",
    "    total_norm = torch.norm(torch.stack(norms), 2)\n",
    "    return float(total_norm.item())\n",
    "\n",
    "def train_one_epoch(model, criterion, data_loader, optimizer, epoch, config):\n",
    "    \"\"\"Train for a single epoch with detailed diagnostics.\"\"\"\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    loss_sums = defaultdict(float)\n",
    "    grad_norm_total = 0.0\n",
    "    grad_norm_count = 0\n",
    "    num_batches = len(data_loader)\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=f\"Epoch {epoch+1}\", leave=False)):\n",
    "        model_inputs, targets = prepare_batch_inputs(batch[1], config.device)\n",
    "\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "            epoch_i=epoch,\n",
    "            batch_idx=batch_idx,\n",
    "            train_loader_length=num_batches,\n",
    "            targets=targets,\n",
    "            is_training=True,\n",
    "        )\n",
    "\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weighted_loss = sum(\n",
    "            loss_dict[k] * criterion.weight_dict[k]\n",
    "            for k in loss_dict.keys()\n",
    "            if k in criterion.weight_dict\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "\n",
    "        if config.grad_clip > 0:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "            grad_norm = float(grad_norm.item() if hasattr(grad_norm, \"item\") else grad_norm)\n",
    "        else:\n",
    "            grad_norm = _compute_grad_norm(model.parameters())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += weighted_loss.item()\n",
    "        for key, value in loss_dict.items():\n",
    "            canon = _canonical_loss_key(key)\n",
    "            if canon is not None:\n",
    "                loss_value = value.item() if isinstance(value, torch.Tensor) else float(value)\n",
    "                loss_sums[canon] += loss_value\n",
    "\n",
    "        if grad_norm is not None:\n",
    "            grad_norm_total += grad_norm\n",
    "            grad_norm_count += 1\n",
    "\n",
    "        if (batch_idx + 1) % 1000 == 0 or batch_idx == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start\n",
    "            diagnostics = _format_loss_dict(loss_dict)\n",
    "            print(\n",
    "                f\"  Batch {batch_idx + 1}/{num_batches} | \"\n",
    "                f\"loss={weighted_loss.item():.4f} | grad_norm={grad_norm:.3f} | \"\n",
    "                f\"elapsed={elapsed:.1f}s\"\n",
    "            )\n",
    "            if diagnostics:\n",
    "                print(f\"    {diagnostics}\")\n",
    "\n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    loss_breakdown = {key: loss_sums[key] / max(num_batches, 1) for key in loss_sums}\n",
    "    avg_grad_norm = (grad_norm_total / grad_norm_count) if grad_norm_count > 0 else 0.0\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Epoch {epoch+1} finished in {epoch_time/60:.2f} min\")\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'lr': optimizer.param_groups[0]['lr'],\n",
    "        'loss_breakdown': loss_breakdown,\n",
    "        'grad_norm': avg_grad_norm,\n",
    "        'epoch_time': epoch_time,\n",
    "    }\n",
    "\n",
    "print(\"Training loop helper functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc6b96",
   "metadata": {},
   "source": [
    "## 11. Evaluation and Inference\n",
    "\n",
    "Evaluation metrics for Video Moment Retrieval:\n",
    "- **R@N, IoU=m**: Recall at N predictions with IoU threshold m\n",
    "- **mIoU**: Mean Intersection over Union\n",
    "\n",
    "For Highlight Detection:\n",
    "- **mAP**: Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data_loader, config):\n",
    "    \"\"\"\n",
    "    Run inference on validation/test set\n",
    "    \n",
    "    Args:\n",
    "        model: Switch-Net model (alias LD_DETR remains supported)\n",
    "        data_loader: Validation/test data loader\n",
    "        config: Configuration object\n",
    "    \n",
    "    Returns:\n",
    "        predictions: List of predictions for each query\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Inference\")):\n",
    "            # Prepare batch inputs\n",
    "            model_inputs, targets = prepare_batch_inputs(batch[1], config.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                **model_inputs,\n",
    "                epoch_i=0,\n",
    "                batch_idx=batch_idx,\n",
    "                train_loader_length=len(data_loader),\n",
    "                targets=None,\n",
    "                is_training=False\n",
    "            )\n",
    "            \n",
    "            # Extract predictions\n",
    "            pred_logits = outputs[\"pred_logits\"]  # (batch_size, num_queries, 2)\n",
    "            pred_spans = outputs[\"pred_spans\"]    # (batch_size, num_queries, 2)\n",
    "            saliency_scores = outputs[\"saliency_scores\"]  # (batch_size, max_v_l)\n",
    "            \n",
    "            # Convert predictions to numpy\n",
    "            batch_predictions = {\n",
    "                \"pred_logits\": pred_logits.cpu().numpy(),\n",
    "                \"pred_spans\": pred_spans.cpu().numpy(),\n",
    "                \"saliency_scores\": saliency_scores.cpu().numpy(),\n",
    "                \"video_ids\": batch[0]  # Assuming batch contains video IDs\n",
    "            }\n",
    "            predictions.append(batch_predictions)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23bf945",
   "metadata": {},
   "source": [
    "## 12. Complete Training Script\n",
    "\n",
    "This is the main training loop that brings everything together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f8555",
   "metadata": {},
   "source": [
    "## Dataset and Training\n",
    "\n",
    "**Current Setup (CLIP only):**\n",
    "- Video features: 512-dim CLIP\n",
    "- Query features: 512-dim CLIP (truncated to 32 tokens)\n",
    "- Model vid_dim: 512\n",
    "\n",
    "**Future: Adding SlowFast features:**\n",
    "1. Extract SlowFast features to `/data/features/video_slowfast_features/`\n",
    "2. Update VIDEO_FEAT_DIRS to list: `[clip_dir, slowfast_dir]`\n",
    "3. Update `config.vid_dim = 2816` (512 + 2304)\n",
    "4. Re-run model initialization\n",
    "5. Features will be automatically concatenated during data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Dataset for video moment retrieval with TEF augmentation and saliency supervision.\"\"\"\n",
    "    def __init__(self, data_path, video_feat_dirs, query_feat_dir, \n",
    "                 max_v_l=75, max_q_l=32, clip_len=2, normalize_features=True,\n",
    "                 saliency_pairs=5, saliency_value_scale=12.0, min_span_width_ratio=0.0):\n",
    "        self.data_path = data_path\n",
    "        self.video_feat_dirs = video_feat_dirs if isinstance(video_feat_dirs, list) else [video_feat_dirs]\n",
    "        self.query_feat_dir = query_feat_dir\n",
    "        self.max_v_l = max_v_l if (max_v_l is not None and max_v_l > 0) else None\n",
    "        self.max_q_l = max_q_l\n",
    "        self.clip_len = clip_len\n",
    "        self.normalize_features = normalize_features\n",
    "        self.saliency_pairs = saliency_pairs\n",
    "        self.saliency_value_scale = float(saliency_value_scale)\n",
    "        self.min_span_width_ratio = float(max(min_span_width_ratio, 0.0))\n",
    "        \n",
    "        self.data = []\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.data.append(json.loads(line.strip()))\n",
    "        \n",
    "        self.feature_dim = None\n",
    "        self.query_dim = None\n",
    "        self._infer_feature_dims()\n",
    "        if self.max_v_l is None:\n",
    "            self.max_v_l = self._infer_max_context_length()\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} samples from {data_path}\")\n",
    "        print(f\"[VideoDataset] video_dim={self.feature_dim}, query_dim={self.query_dim}, normalize={self.normalize_features}\")\n",
    "        print(f\"[VideoDataset] max_v_l={self.max_v_l}, saliency scale={self.saliency_value_scale}, min_span_width_ratio={self.min_span_width_ratio}\")\n",
    "        \n",
    "    def _infer_feature_dims(self):\n",
    "        for item in self.data:\n",
    "            vid = item.get('vid')\n",
    "            qid = item.get('qid')\n",
    "            dims = 0\n",
    "            for feat_dir in self.video_feat_dirs:\n",
    "                npz_path = os.path.join(feat_dir, f\"{vid}.npz\")\n",
    "                npy_path = os.path.join(feat_dir, f\"{vid}.npy\")\n",
    "                if os.path.exists(npz_path):\n",
    "                    with np.load(npz_path) as arr:\n",
    "                        dims += arr['features'].shape[-1]\n",
    "                elif os.path.exists(npy_path):\n",
    "                    dims += np.load(npy_path).shape[-1]\n",
    "            if dims > 0:\n",
    "                self.feature_dim = dims + 2  # +2 for TEF channels\n",
    "            q_path = os.path.join(self.query_feat_dir, f\"{qid}.npz\")\n",
    "            if os.path.exists(q_path):\n",
    "                with np.load(q_path) as q_arr:\n",
    "                    self.query_dim = q_arr['last_hidden_state'].shape[-1]\n",
    "            if self.feature_dim is not None and self.query_dim is not None:\n",
    "                break\n",
    "        if self.feature_dim is None:\n",
    "            self.feature_dim = 512 * len(self.video_feat_dirs) + 2\n",
    "        if self.query_dim is None:\n",
    "            self.query_dim = 512\n",
    "        \n",
    "    def _infer_max_context_length(self):\n",
    "        \"\"\"Infer the maximum temporal length across all videos.\"\"\"\n",
    "        max_len = 0\n",
    "        for item in self.data:\n",
    "            vid = item.get('vid')\n",
    "            lengths = []\n",
    "            for feat_dir in self.video_feat_dirs:\n",
    "                npz_path = os.path.join(feat_dir, f\"{vid}.npz\")\n",
    "                npy_path = os.path.join(feat_dir, f\"{vid}.npy\")\n",
    "                if os.path.exists(npz_path):\n",
    "                    with np.load(npz_path) as arr:\n",
    "                        lengths.append(arr['features'].shape[0])\n",
    "                elif os.path.exists(npy_path):\n",
    "                    lengths.append(np.load(npy_path).shape[0])\n",
    "            if lengths:\n",
    "                max_len = max(max_len, min(lengths))\n",
    "        return int(max_len or 1)\n",
    "\n",
    "    # Build Temporal Encoding Features (TEF)\n",
    "    def _build_tef(self, length):\n",
    "        if length <= 0:\n",
    "            return np.zeros((0, 2), dtype=np.float32)\n",
    "        positions = np.arange(length, dtype=np.float32)\n",
    "        denom = float(max(length, 1))\n",
    "        tef_start = positions / denom\n",
    "        tef_end = np.minimum((positions + 1.0) / denom, 1.0)\n",
    "        return np.stack([tef_start, tef_end], axis=1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _generate_saliency_labels(self, spans, duration, video_len):\n",
    "        scores = np.zeros(self.max_v_l, dtype=np.float32)\n",
    "        if video_len <= 0 or duration <= 0 or not spans:\n",
    "            filler = np.zeros(self.saliency_pairs, dtype=np.int64)\n",
    "            return filler, filler.copy(), scores\n",
    "        clip_length = duration / max(video_len, 1)\n",
    "        pos_indices = set()\n",
    "        for start, end in spans:\n",
    "            st_idx = int(np.floor(start / clip_length))\n",
    "            ed_idx = int(np.ceil(end / clip_length)) - 1\n",
    "            st_idx = max(0, min(st_idx, video_len - 1))\n",
    "            ed_idx = max(st_idx, min(ed_idx, video_len - 1))\n",
    "            if ed_idx >= st_idx:\n",
    "                pos_indices.update(range(st_idx, ed_idx + 1))\n",
    "                scores[st_idx:ed_idx + 1] = self.saliency_value_scale\n",
    "        neg_pool = [i for i in range(video_len) if i not in pos_indices]\n",
    "        if not pos_indices:\n",
    "            pos_indices = {0}\n",
    "        if not neg_pool:\n",
    "            neg_pool = list(pos_indices)\n",
    "        pos_indices = list(pos_indices)\n",
    "        num_pairs = min(self.saliency_pairs, len(pos_indices), len(neg_pool))\n",
    "        if num_pairs == 0:\n",
    "            num_pairs = 1\n",
    "        pos_samples = np.random.choice(pos_indices, size=num_pairs, replace=len(pos_indices) < num_pairs)\n",
    "        neg_samples = np.random.choice(neg_pool, size=num_pairs, replace=len(neg_pool) < num_pairs)\n",
    "        pos_array = np.full(self.saliency_pairs, pos_samples[0], dtype=np.int64)\n",
    "        neg_array = np.full(self.saliency_pairs, neg_samples[0], dtype=np.int64)\n",
    "        pos_array[:num_pairs] = pos_samples\n",
    "        neg_array[:num_pairs] = neg_samples\n",
    "        return pos_array, neg_array, scores\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        qid = item['qid']\n",
    "        vid = item['vid']\n",
    "        duration = float(item.get('duration', 0.0))\n",
    "        relevant_windows = item.get('relevant_windows', [])\n",
    "        duration = max(duration, 1e-6)\n",
    "        \n",
    "        try:\n",
    "            v_feat_list = []\n",
    "            for feat_dir in self.video_feat_dirs:\n",
    "                npz_path = os.path.join(feat_dir, f\"{vid}.npz\")\n",
    "                npy_path = os.path.join(feat_dir, f\"{vid}.npy\")\n",
    "                if os.path.exists(npz_path):\n",
    "                    with np.load(npz_path) as arr:\n",
    "                        _feat = arr['features'].astype(np.float32)\n",
    "                elif os.path.exists(npy_path):\n",
    "                    _feat = np.load(npy_path).astype(np.float32)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Missing features for {vid} in {feat_dir}\")\n",
    "                if self.max_v_l is not None:\n",
    "                    _feat = _feat[:self.max_v_l]\n",
    "                if self.normalize_features:\n",
    "                    norms = np.linalg.norm(_feat, axis=1, keepdims=True) + 1e-6\n",
    "                    _feat = _feat / norms\n",
    "                v_feat_list.append(_feat)\n",
    "            min_len = min(len(e) for e in v_feat_list)\n",
    "            v_feat_list = [e[:min_len] for e in v_feat_list]\n",
    "            video_feat = np.concatenate(v_feat_list, axis=1)\n",
    "            tef = self._build_tef(video_feat.shape[0])\n",
    "            video_feat = np.concatenate([video_feat, tef], axis=1)\n",
    "            self.feature_dim = video_feat.shape[-1]\n",
    "        except Exception as exc:\n",
    "            print(f\"Warning: Could not load video features for vid={vid} - {exc}\")\n",
    "            fallback_len = int(self.max_v_l or 1)\n",
    "            fallback_dim = int(self.feature_dim or (len(self.video_feat_dirs) * 512 + 2))\n",
    "            video_feat = np.zeros((fallback_len, fallback_dim), dtype=np.float32)\n",
    "        \n",
    "        try:\n",
    "            with np.load(os.path.join(self.query_feat_dir, f\"{qid}.npz\")) as query_data:\n",
    "                query_feat = query_data['last_hidden_state'].astype(np.float32)[:self.max_q_l]\n",
    "            if self.normalize_features:\n",
    "                norms = np.linalg.norm(query_feat, axis=1, keepdims=True) + 1e-6\n",
    "                query_feat = query_feat / norms\n",
    "            self.query_dim = query_feat.shape[-1]\n",
    "        except Exception as exc:\n",
    "            print(f\"Warning: Could not load query features for qid={qid} - {exc}\")\n",
    "            fallback_query_dim = int(self.query_dim or 512)\n",
    "            query_feat = np.zeros((self.max_q_l, fallback_query_dim), dtype=np.float32)\n",
    "        \n",
    "        spans = []\n",
    "        for start, end in relevant_windows:\n",
    "            center = (start + end) / (2 * duration)\n",
    "            width = max(end - start, 1e-6) / duration\n",
    "            spans.append([center, width])\n",
    "        spans = np.array(spans, dtype=np.float32) if spans else np.zeros((0, 2), dtype=np.float32)\n",
    "        \n",
    "        video_len = min(video_feat.shape[0], self.max_v_l)\n",
    "        query_len = min(query_feat.shape[0], self.max_q_l)\n",
    "        video_mask = np.zeros(self.max_v_l, dtype=np.float32)\n",
    "        video_mask[:video_len] = 1\n",
    "        query_mask = np.zeros(self.max_q_l, dtype=np.float32)\n",
    "        query_mask[:query_len] = 1\n",
    "        \n",
    "        video_feat_padded = np.zeros((self.max_v_l, video_feat.shape[-1]), dtype=np.float32)\n",
    "        video_feat_padded[:video_len] = video_feat[:video_len]\n",
    "        query_feat_padded = np.zeros((self.max_q_l, query_feat.shape[-1]), dtype=np.float32)\n",
    "        query_feat_padded[:query_len] = query_feat[:query_len]\n",
    "        \n",
    "        saliency_pos, saliency_neg, saliency_all = self._generate_saliency_labels(\n",
    "            relevant_windows, duration, video_len)\n",
    "        \n",
    "        return {\n",
    "            'qid': qid,\n",
    "            'vid': vid,\n",
    "            'video_feat': video_feat_padded,\n",
    "            'query_feat': query_feat_padded,\n",
    "            'video_mask': video_mask,\n",
    "            'query_mask': query_mask,\n",
    "            'spans': spans,\n",
    "            'duration': duration,\n",
    "            'saliency_pos_labels': saliency_pos,\n",
    "            'saliency_neg_labels': saliency_neg,\n",
    "            'saliency_all_labels': saliency_all\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for batching.\"\"\"\n",
    "    video_feats = torch.from_numpy(np.stack([item['video_feat'] for item in batch])).float()\n",
    "    query_feats = torch.from_numpy(np.stack([item['query_feat'] for item in batch])).float()\n",
    "    video_masks = torch.from_numpy(np.stack([item['video_mask'] for item in batch])).float()\n",
    "    query_masks = torch.from_numpy(np.stack([item['query_mask'] for item in batch])).float()\n",
    "    \n",
    "    span_labels = []\n",
    "    for item in batch:\n",
    "        spans_np = item['spans']\n",
    "        spans_tensor = torch.from_numpy(spans_np).float() if spans_np.size > 0 else torch.zeros((0, 2))\n",
    "        span_labels.append({'spans': spans_tensor})\n",
    "    \n",
    "    targets = {\n",
    "        'span_labels': span_labels,\n",
    "        'saliency_pos_labels': torch.from_numpy(np.stack([item['saliency_pos_labels'] for item in batch])).long(),\n",
    "        'saliency_neg_labels': torch.from_numpy(np.stack([item['saliency_neg_labels'] for item in batch])).long(),\n",
    "        'saliency_all_labels': torch.from_numpy(np.stack([item['saliency_all_labels'] for item in batch])).float(),\n",
    "    }\n",
    "    \n",
    "    model_inputs = {\n",
    "        'src_vid': video_feats,\n",
    "        'src_txt': query_feats,\n",
    "        'src_vid_mask': video_masks,\n",
    "        'src_txt_mask': query_masks,\n",
    "    }\n",
    "    \n",
    "    return {'meta': [item['vid'] for item in batch]}, (model_inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac56767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "try:\n",
    "    train_dataset = VideoDataset(\n",
    "        data_path=TRAIN_PATH,\n",
    "        video_feat_dirs=VIDEO_FEAT_DIRS,  # Supports multiple feature types\n",
    "        query_feat_dir=QUERY_FEAT_DIR,\n",
    "        max_v_l=config.max_v_l,\n",
    "        max_q_l=config.max_q_l,\n",
    "        clip_len=config.clip_len,\n",
    "        normalize_features=True,\n",
    "        saliency_value_scale=config.saliency_label_scale,\n",
    "        min_span_width_ratio=config.min_span_width_ratio,\n",
    "    )\n",
    "    \n",
    "    # Sync config with inferred feature dimensions\n",
    "    config.max_v_l = train_dataset.max_v_l\n",
    "    config.vid_dim = train_dataset.feature_dim\n",
    "    config.txt_dim = train_dataset.query_dim\n",
    "    print(f\"Updated config feature dims -> vid_dim={config.vid_dim}, txt_dim={config.txt_dim}\")\n",
    "    print(f\"Updated config max_v_l -> {config.max_v_l}\")\n",
    "    \n",
    "    # Ensure model/criterion reflect the updated temporal length\n",
    "    if 'model' in globals():\n",
    "        model.max_v_l = config.max_v_l\n",
    "    if 'criterion' in globals():\n",
    "        criterion.max_v_l = config.max_v_l\n",
    "    \n",
    "    test_dataset = VideoDataset(\n",
    "        data_path=TEST_PATH,\n",
    "        video_feat_dirs=VIDEO_FEAT_DIRS,\n",
    "        query_feat_dir=QUERY_FEAT_DIR,\n",
    "        max_v_l=config.max_v_l,\n",
    "        max_q_l=config.max_q_l,\n",
    "        clip_len=config.clip_len,\n",
    "        normalize_features=True,\n",
    "        saliency_value_scale=config.saliency_label_scale,\n",
    "        min_span_width_ratio=config.min_span_width_ratio,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDatasets created successfully!\")\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for debugging, increase for faster loading\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataloaders created successfully!\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Test loading one batch\n",
    "    print(\"\\nTesting data loading...\")\n",
    "    meta, (model_inputs, targets) = next(iter(train_loader))\n",
    "    print(f\"Video features shape: {model_inputs['src_vid'].shape}\")\n",
    "    print(f\"Query features shape: {model_inputs['src_txt'].shape}\")\n",
    "    print(f\"Video mask shape: {model_inputs['src_vid_mask'].shape}\")\n",
    "    print(f\"Query mask shape: {model_inputs['src_txt_mask'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating datasets or dataloaders: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777a517",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Video Moment Retrieval\n",
    "\n",
    "This section implements **standard evaluation metrics** used in video moment retrieval:\n",
    "\n",
    "1. **Temporal IoU (Intersection over Union)**\n",
    "   - Measures overlap between predicted and ground truth spans\n",
    "   - Formula: `IoU = (intersection) / (union)`\n",
    "   - Range: [0, 1], where 1 = perfect match\n",
    "\n",
    "2. **Recall at N (R@N, IoU≥θ)**\n",
    "   - Percentage of queries with at least one correct prediction in top-N\n",
    "   - Common settings: R@1, R@5, R@10 with IoU≥0.5 or IoU≥0.7\n",
    "   - Example: R@5, IoU≥0.5 = \"Top-5 predictions contain at least one span with IoU ≥ 0.5\"\n",
    "\n",
    "3. **mean Average Precision (mAP @ IoU≥θ)**\n",
    "   - Average precision across all queries at a given IoU threshold\n",
    "   - Considers ranking quality (better predictions ranked higher = higher mAP)\n",
    "   - Common thresholds: IoU≥0.5, IoU≥0.7\n",
    "\n",
    "These metrics are **essential for evaluating moment retrieval quality** and comparing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f39718",
   "metadata": {},
   "source": [
    "# Evaluation Functions\n",
    "\n",
    "Functions to compute retrieval metrics (mAP, IoU, R@N) for moment retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_single(pred_span, gt_span):\n",
    "    \"\"\"\n",
    "    Simple IoU computation for two spans (reuses logic from generalized_temporal_iou).\n",
    "    \n",
    "    Args:\n",
    "        pred_span: [start, end] predicted span\n",
    "        gt_span: [start, end] ground truth span\n",
    "    \n",
    "    Returns:\n",
    "        iou: float in [0, 1]\n",
    "    \"\"\"\n",
    "    inter_start = max(pred_span[0], gt_span[0])\n",
    "    inter_end = min(pred_span[1], gt_span[1])\n",
    "    inter = max(0, inter_end - inter_start)\n",
    "    \n",
    "    union = (pred_span[1] - pred_span[0]) + (gt_span[1] - gt_span[0]) - inter\n",
    "    \n",
    "    if union <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def evaluate_moment_retrieval(model, dataloader, config, device, iou_thresholds=[0.3, 0.5, 0.7]):\n",
    "    \"\"\"\n",
    "    Evaluate moment retrieval performance on test/validation set.\n",
    "    \n",
    "    Computes:\n",
    "    - mIoU: Mean IoU of the top-1 ranked prediction for each query\n",
    "    - R@N, IoU≥θ: Recall at top-N predictions for each IoU threshold\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Switch-Net model (legacy LD_DETR alias supported)\n",
    "        dataloader: DataLoader for evaluation\n",
    "        config: Configuration object\n",
    "        device: Device to run evaluation on\n",
    "        iou_thresholds: List of IoU thresholds for R@N metrics\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary with mIoU, R@1, R@5, R@10 at different IoU thresholds\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_top1_ious = []\n",
    "    recall_hits = {thr: {'R@1': 0, 'R@5': 0, 'R@10': 0} for thr in iou_thresholds}\n",
    "    total_queries = 0\n",
    "    loader_length = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            _, batch_data = batch\n",
    "            model_inputs, targets = prepare_batch_inputs(batch_data, device)\n",
    "            \n",
    "            outputs = model(\n",
    "                **model_inputs,\n",
    "                epoch_i=0,\n",
    "                batch_idx=0,\n",
    "                train_loader_length=loader_length,\n",
    "                targets=None,\n",
    "                is_training=False,\n",
    "            )\n",
    "            \n",
    "            pred_logits = outputs['pred_logits']  # (batch_size, num_queries, num_classes)\n",
    "            pred_spans_cxw = outputs['pred_spans']  # (batch_size, num_queries, 2) in (center, width)\n",
    "            pred_spans_xx = span_cxw_to_xx(pred_spans_cxw)  # Convert to (start, end) for IoU\n",
    "            target_list = targets['span_labels']\n",
    "            batch_size = pred_spans_xx.shape[0]\n",
    "            \n",
    "            for sample_idx in range(batch_size):\n",
    "                if sample_idx >= len(target_list):\n",
    "                    continue\n",
    "                \n",
    "                gt_spans_cxw = target_list[sample_idx]['spans']\n",
    "                if gt_spans_cxw.numel() == 0:\n",
    "                    continue\n",
    "                \n",
    "                gt_spans_xx = span_cxw_to_xx(gt_spans_cxw)\n",
    "                gt_spans_np = gt_spans_xx.detach().cpu().numpy()\n",
    "                if gt_spans_np.ndim == 1:\n",
    "                    gt_spans_np = gt_spans_np[None, :]\n",
    "                \n",
    "                sample_spans = pred_spans_xx[sample_idx].detach().cpu().numpy()\n",
    "                if sample_spans.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                sample_logits = pred_logits[sample_idx]\n",
    "                probs = F.softmax(sample_logits, dim=-1)\n",
    "                if probs.shape[-1] == 1:\n",
    "                    scores = probs.squeeze(-1)\n",
    "                else:\n",
    "                    scores = probs[:, 0]\n",
    "                scores_np = scores.detach().cpu().numpy()\n",
    "                sorted_indices = np.argsort(scores_np)[::-1]\n",
    "                sorted_spans = sample_spans[sorted_indices]\n",
    "                \n",
    "                per_pred_best_ious = []\n",
    "                for pred_span in sorted_spans:\n",
    "                    ious_to_gts = [compute_iou_single(pred_span, gt_span) for gt_span in gt_spans_np]\n",
    "                    per_pred_best_ious.append(max(ious_to_gts) if ious_to_gts else 0.0)\n",
    "                \n",
    "                if not per_pred_best_ious:\n",
    "                    continue\n",
    "                \n",
    "                top1_iou = per_pred_best_ious[0]\n",
    "                all_top1_ious.append(top1_iou)\n",
    "                \n",
    "                for thr in iou_thresholds:\n",
    "                    if top1_iou >= thr:\n",
    "                        recall_hits[thr]['R@1'] += 1\n",
    "                    top5_hits = any(iou >= thr for iou in per_pred_best_ious[:5])\n",
    "                    if top5_hits:\n",
    "                        recall_hits[thr]['R@5'] += 1\n",
    "                    top10_hits = any(iou >= thr for iou in per_pred_best_ious[:10])\n",
    "                    if top10_hits:\n",
    "                        recall_hits[thr]['R@10'] += 1\n",
    "                \n",
    "                total_queries += 1\n",
    "    \n",
    "    metrics = {'mIoU': float(np.mean(all_top1_ious)) if all_top1_ious else 0.0}\n",
    "    \n",
    "    if total_queries > 0:\n",
    "        for thr in iou_thresholds:\n",
    "            metrics[f'R@1_IoU{thr}'] = recall_hits[thr]['R@1'] / total_queries * 100\n",
    "            metrics[f'R@5_IoU{thr}'] = recall_hits[thr]['R@5'] / total_queries * 100\n",
    "            metrics[f'R@10_IoU{thr}'] = recall_hits[thr]['R@10'] / total_queries * 100\n",
    "    else:\n",
    "        for thr in iou_thresholds:\n",
    "            metrics[f'R@1_IoU{thr}'] = 0.0\n",
    "            metrics[f'R@5_IoU{thr}'] = 0.0\n",
    "            metrics[f'R@10_IoU{thr}'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_metrics(metrics, prefix=\"\"):\n",
    "    \"\"\"Pretty print evaluation metrics.\"\"\"\n",
    "    print(f\"\\n{prefix}Evaluation Metrics:\")\n",
    "    print(f\"  mIoU: {metrics['mIoU']:.4f}\")\n",
    "    \n",
    "    for thr in [0.3, 0.5, 0.7]:\n",
    "        if f'R@1_IoU{thr}' in metrics:\n",
    "            print(f\"  IoU≥{thr}:\")\n",
    "            print(f\"    R@1:  {metrics[f'R@1_IoU{thr}']:.2f}%\")\n",
    "            print(f\"    R@5:  {metrics[f'R@5_IoU{thr}']:.2f}%\")\n",
    "            print(f\"    R@10: {metrics[f'R@10_IoU{thr}']:.2f}%\")\n",
    "\n",
    "print(\"Evaluation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3794023",
   "metadata": {},
   "source": [
    "# Training Pipeline with Checkpointing\n",
    "\n",
    "Complete training pipeline with:\n",
    "- Model checkpointing (best, latest, final)\n",
    "- Results logging (CSV format)\n",
    "- Early stopping mechanism\n",
    "- Periodic evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR = 'output'\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models')\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, 'results')\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directories created:\")\n",
    "print(f\"  Models: {MODELS_DIR}\")\n",
    "print(f\"  Results: {RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, filename, config):\n",
    "    \"\"\"Save model checkpoint to disk.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics or {},\n",
    "        'config': vars(config)\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"  Checkpoint saved to {filename}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    \"\"\"Load a previously saved checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint.get('metrics', {})\n",
    "\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"CSV logger with rich training / evaluation diagnostics.\"\"\"\n",
    "\n",
    "    LOSS_KEYS = [\n",
    "        'loss_span',\n",
    "        'loss_giou',\n",
    "        'loss_label',\n",
    "        'loss_saliency',\n",
    "        'loss_align',\n",
    "        'loss_sim',\n",
    "        'loss_moe_balance',\n",
    "        'class_error',\n",
    "    ]\n",
    "\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.results = []\n",
    "\n",
    "        self.fieldnames = [\n",
    "            'timestamp', 'epoch', 'train_loss', 'lr', 'grad_norm',\n",
    "        ] + self.LOSS_KEYS + [\n",
    "            'mIoU', 'R@1_IoU0.3', 'R@5_IoU0.3', 'R@10_IoU0.3',\n",
    "            'R@1_IoU0.5', 'R@5_IoU0.5', 'R@10_IoU0.5',\n",
    "            'R@1_IoU0.7', 'R@5_IoU0.7', 'R@10_IoU0.7'\n",
    "        ]\n",
    "\n",
    "        with open(log_file, 'w', newline='') as f:\n",
    "            csv.DictWriter(f, fieldnames=self.fieldnames).writeheader()\n",
    "\n",
    "    def log(self, epoch, train_stats, eval_metrics=None):\n",
    "        \"\"\"Append a line of metrics to the CSV log.\"\"\"\n",
    "        result = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'epoch': epoch,\n",
    "            'train_loss': float(train_stats.get('loss', 0.0)),\n",
    "            'lr': float(train_stats.get('lr', 0.0)),\n",
    "            'grad_norm': float(train_stats.get('grad_norm', 0.0) or 0.0),\n",
    "        }\n",
    "\n",
    "        loss_breakdown = train_stats.get('loss_breakdown', {}) or {}\n",
    "        for key in self.LOSS_KEYS:\n",
    "            result[key] = float(loss_breakdown.get(key, 0.0))\n",
    "\n",
    "        if eval_metrics:\n",
    "            for key in self.fieldnames[5 + len(self.LOSS_KEYS):]:\n",
    "                result[key] = float(eval_metrics.get(key, 0.0))\n",
    "\n",
    "        with open(self.log_file, 'a', newline='') as f:\n",
    "            csv.DictWriter(f, fieldnames=self.fieldnames).writerow(result)\n",
    "\n",
    "        self.results.append(result)\n",
    "\n",
    "    def save_summary(self, summary_file):\n",
    "        \"\"\"Persist accumulated training history as JSON.\"\"\"\n",
    "        summary = {\n",
    "            'total_epochs': len(self.results),\n",
    "            'best_miou': max((r.get('mIoU', 0.0) for r in self.results), default=0.0),\n",
    "            'final_loss': self.results[-1]['train_loss'] if self.results else 0.0,\n",
    "            'training_history': self.results\n",
    "        }\n",
    "\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        print(f\"Training summary saved to {summary_file}\")\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping handler.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0.001, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_value = None\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, current_value):\n",
    "        if self.best_value is None:\n",
    "            self.best_value = current_value\n",
    "            return False\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            improved = (self.best_value - current_value) > self.min_delta\n",
    "        else:\n",
    "            improved = (current_value - self.best_value) > self.min_delta\n",
    "\n",
    "        if improved:\n",
    "            self.best_value = current_value\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "                print(f\"\\nEarly stopping triggered! No improvement for {self.patience} epochs.\")\n",
    "        return self.should_stop\n",
    "\n",
    "\n",
    "print(\"Training utilities initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d595ad4",
   "metadata": {},
   "source": [
    "### Training Orchestration\n",
    "\n",
    "The next cell drives the full training schedule: it initialises logging, steps the scheduler, triggers periodic evaluation, and manages checkpoint persistence (best/latest/final). Run it after all preceding definitions have executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebfb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = config.n_epoch\n",
    "eval_every = 1  # Evaluate every N epochs\n",
    "use_early_stopping = False  # Set to True to enable early stopping\n",
    "early_stop_patience = 5\n",
    "\n",
    "# Weights & Biases experiment tracking\n",
    "use_wandb = True  # Set to False to disable logging to Weights & Biases\n",
    "wandb_project = \"Switch-net-moment-retrieval\"  # Update this to your own W&B project name if needed\n",
    "wandb_run = None\n",
    "if use_wandb and wandb is None:\n",
    "    print(\"Weights & Biases module not found. Install it with `pip install wandb` or set use_wandb=False.\")\n",
    "    use_wandb = False\n",
    "\n",
    "# Initialize logger\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = os.path.join(RESULTS_DIR, f'training_log_{timestamp}.csv')\n",
    "logger = TrainingLogger(log_file)\n",
    "\n",
    "# Initialize early stopping (optional)\n",
    "if use_early_stopping:\n",
    "    early_stopping = EarlyStopping(patience=early_stop_patience, mode='max')  # Monitor mIoU\n",
    "    print(f\"Early stopping enabled with patience={early_stop_patience}\")\n",
    "\n",
    "# Optionally initialise Weights & Biases\n",
    "if use_wandb:\n",
    "    wandb_run_name = f\"Switch-net_{timestamp}\"\n",
    "    initial_lr = optimizer.param_groups[0].get('lr', None) if optimizer.param_groups else None\n",
    "    wandb_config = {\n",
    "        \"model\": \"Switch-net\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"eval_every\": eval_every,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"learning_rate\": initial_lr,\n",
    "        \"batch_size\": getattr(config, 'batch_size', None),\n",
    "        \"dataset\": getattr(config, 'dataset_name', None),\n",
    "        \"timestamp\": timestamp,\n",
    "    }\n",
    "    try:\n",
    "        wandb_run = wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run_name,\n",
    "            config={k: v for k, v in wandb_config.items() if v is not None},\n",
    "            reinit=True,\n",
    "        )\n",
    "        wandb_run.define_metric(\"epoch\")\n",
    "        wandb_run.define_metric(\"train/*\", step_metric=\"epoch\")\n",
    "        wandb_run.define_metric(\"val/*\", step_metric=\"epoch\")\n",
    "        print(f\"W&B logging enabled -> project='{wandb_project}', run='{wandb_run_name}'\")\n",
    "    except Exception as exc:\n",
    "        print(f\"WARNING: W&B initialisation failed: {exc}\")\n",
    "        print(\"Continuing without W&B logging. Set use_wandb=False to silence this message.\")\n",
    "        wandb_run = None\n",
    "        use_wandb = False\n",
    "\n",
    "# Track best model\n",
    "best_miou = 0.0\n",
    "best_epoch = 0\n",
    "best_checkpoint_path = None\n",
    "completed_epochs = 0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting Training\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total epochs: {num_epochs}\")\n",
    "print(f\"Evaluation frequency: every {eval_every} epochs\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "print(f\"Models will be saved to: {MODELS_DIR}\")\n",
    "if use_wandb:\n",
    "    print(\"Metrics will also be logged to Weights & Biases.\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    train_stats = train_one_epoch(model, criterion, train_loader, optimizer, epoch, config)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"\\nTraining Stats:\")\n",
    "    print(f\"  Loss: {train_stats['loss']:.4f}\")\n",
    "    print(f\"  Learning Rate: {train_stats['lr']:.6f}\")\n",
    "    if train_stats.get('epoch_time') is not None:\n",
    "        print(f\"  Epoch time: {train_stats['epoch_time'] / 60:.2f} min\")\n",
    "    if train_stats.get('grad_norm') is not None:\n",
    "        print(f\"  Avg Grad Norm: {train_stats['grad_norm']:.4f}\")\n",
    "    if train_stats.get('saliency_warmup_active'):\n",
    "        print(\"  Saliency warm-up active -> loss_saliency weight set to 0 this epoch\")\n",
    "    applied_loss_weights = train_stats.get('applied_loss_weights')\n",
    "    if applied_loss_weights:\n",
    "        print(\"  Applied loss weights:\")\n",
    "        for key in sorted(applied_loss_weights):\n",
    "            print(f\"    {key}: {applied_loss_weights[key]:.3f}\")\n",
    "    if train_stats.get('loss_breakdown'):\n",
    "        print(\"  Loss Breakdown:\")\n",
    "        for key in sorted(train_stats['loss_breakdown']):\n",
    "            print(f\"    {key}: {train_stats['loss_breakdown'][key]:.4f}\")\n",
    "\n",
    "    if use_wandb and wandb_run is not None:\n",
    "        wandb_log = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/loss\": float(train_stats.get('loss', 0.0)),\n",
    "            \"train/lr\": float(train_stats.get('lr', 0.0)),\n",
    "        }\n",
    "        if train_stats.get('grad_norm') is not None:\n",
    "            wandb_log[\"train/grad_norm\"] = float(train_stats['grad_norm'])\n",
    "        if train_stats.get('epoch_time') is not None:\n",
    "            wandb_log[\"train/epoch_minutes\"] = float(train_stats['epoch_time'] / 60.0)\n",
    "        loss_breakdown = train_stats.get('loss_breakdown') or {}\n",
    "        for key, value in loss_breakdown.items():\n",
    "            wandb_log[f\"train/{key}\"] = float(value)\n",
    "        for key, value in (applied_loss_weights or {}).items():\n",
    "            wandb_log[f\"train/weight_{key}\"] = float(value)\n",
    "        wandb.log(wandb_log, step=epoch + 1)\n",
    "\n",
    "    eval_metrics = None\n",
    "    should_eval = ((epoch + 1) % eval_every == 0) or ((epoch + 1) == num_epochs)\n",
    "    if should_eval:\n",
    "        print(\"\\nRunning evaluation...\")\n",
    "        eval_metrics = evaluate_moment_retrieval(\n",
    "            model,\n",
    "            test_loader,\n",
    "            config,\n",
    "            device,\n",
    "            iou_thresholds=[0.3, 0.5, 0.7],\n",
    "        )\n",
    "        print_metrics(eval_metrics, prefix=\"Validation \")\n",
    "\n",
    "        current_miou = eval_metrics.get('mIoU', 0.0)\n",
    "        if current_miou > best_miou:\n",
    "            best_miou = current_miou\n",
    "            best_epoch = epoch + 1\n",
    "            best_checkpoint_path = os.path.join(MODELS_DIR, 'best_model_moe.pth')\n",
    "            save_checkpoint(model, optimizer, epoch + 1, eval_metrics, best_checkpoint_path, config)\n",
    "            print(f\"  New best model saved (mIoU={current_miou:.4f})\")\n",
    "            if use_wandb and wandb_run is not None:\n",
    "                wandb_run.summary['best_miou'] = best_miou\n",
    "                wandb_run.summary['best_epoch'] = best_epoch\n",
    "\n",
    "    latest_checkpoint_path = os.path.join(MODELS_DIR, 'latest_model_moe.pth')\n",
    "    save_checkpoint(model, optimizer, epoch + 1, eval_metrics or {}, latest_checkpoint_path, config)\n",
    "\n",
    "    if (epoch + 1) == num_epochs:\n",
    "        final_checkpoint_path = os.path.join(MODELS_DIR, f'final_model_epoch{epoch + 1}_moe.pth')\n",
    "        save_checkpoint(model, optimizer, epoch + 1, eval_metrics or {}, final_checkpoint_path, config)\n",
    "\n",
    "    logger.log(epoch + 1, train_stats, eval_metrics)\n",
    "    completed_epochs = epoch + 1\n",
    "\n",
    "    if use_wandb and wandb_run is not None and eval_metrics:\n",
    "        val_log = {\"epoch\": epoch + 1}\n",
    "        for key, value in eval_metrics.items():\n",
    "            val_log[f\"val/{key}\"] = float(value)\n",
    "        wandb.log(val_log, step=epoch + 1)\n",
    "\n",
    "    if use_early_stopping and eval_metrics is not None:\n",
    "        if early_stopping(eval_metrics.get('mIoU', 0.0)):\n",
    "            print(\"\\nEarly stopping criterion met. Stopping training.\")\n",
    "            break\n",
    "\n",
    "summary_file = os.path.join(RESULTS_DIR, f'training_summary_{timestamp}.json')\n",
    "logger.save_summary(summary_file)\n",
    "\n",
    "if best_epoch > 0:\n",
    "    print(f\"\\nBest mIoU {best_miou:.4f} achieved at epoch {best_epoch}.\")\n",
    "else:\n",
    "    print(\"\\nTraining finished without evaluation. Consider increasing eval frequency.\")\n",
    "\n",
    "if use_wandb and wandb_run is not None:\n",
    "    wandb_run.summary.setdefault('best_miou', best_miou)\n",
    "    wandb_run.summary.setdefault('best_epoch', best_epoch)\n",
    "    wandb_run.summary['trained_epochs'] = completed_epochs\n",
    "    wandb_run.summary['log_file'] = log_file\n",
    "    try:\n",
    "        wandb_run.finish()\n",
    "    except Exception:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation with the latest saved checkpoint\n",
    "latest_checkpoint_path = 'output/models/best_model.pth'\n",
    "assert os.path.exists(latest_checkpoint_path), f'Checkpoint not found: {latest_checkpoint_path}'\n",
    "\n",
    "checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Loaded checkpoint from {latest_checkpoint_path}\")\n",
    "\n",
    "quick_metrics = evaluate_moment_retrieval(\n",
    "    model,\n",
    "    test_loader,\n",
    "    config,\n",
    "    device,\n",
    "    iou_thresholds=[0.3, 0.5, 0.7],\n",
    ")\n",
    "print_metrics(quick_metrics, prefix=\"Latest model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae2b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0050192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
